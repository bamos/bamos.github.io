---
layout: index
title: Brandon Amos
---

<p style="text-align: justify;">
I am a person in NYC.
I study <b>safe, intelligent systems</b> that
understand and interact with our world.
A few themes of my research involve:
</p>

<ol>
<li>
  <b>language models</b>
  (on attacks to improve safety and alignment in
  <a href="https://arxiv.org/abs/2404.16873">AdvPrompter</a> and
  <a href="https://arxiv.org/abs/2412.10321">AdvPrefix</a>,
  coding agents in <a href="https://arxiv.org/abs/2507.15887">AlgoTune</a>,
  RL post-training for social value alignment in <a href="https://arxiv.org/abs/2507.09650">community alignment</a>,
  and understanding how to
  <a href="https://arxiv.org/abs/2410.09303">fix broken tokens</a> and
  <a href="https://arxiv.org/abs/2407.18158">improve generalization bounds</a>)
</li>
<li>
  <b>reinforcement learning, control, and game-playing AI</b>
  (in
  <a href="https://arxiv.org/abs/1810.13400">differentiable MPC</a>,
  <a href="https://arxiv.org/abs/1804.06318">learning awareness models</a>,
  <a href="https://arxiv.org/abs/2008.12775">the model-based SVG</a>, and
  with language-based intrinsic rewards in
  <a href="https://arxiv.org/abs/2410.23022">ONI</a>)
</li>
<li>
  <b>applied optimal transport and flows</b>
  (in
  <a href="https://arxiv.org/abs/2106.10272">Riemannian Convex Potential Maps</a>,
  <a href="https://arxiv.org/abs/2110.03684">Gromov-Wasserstein Imitation Learning</a>,
  <a href="https://arxiv.org/abs/2210.12153">amortized convex conjugates</a>,
  <a href="https://arxiv.org/abs/2406.00288">Lagrangian OT</a>,
  <a href="https://arxiv.org/abs/2206.05262">Meta Optimal Transport</a>,
  <a href="https://arxiv.org/abs/2408.14608">Meta Flow Matching</a>,
  <a href="https://arxiv.org/abs/2304.14772">Multisample Flow Matching</a>,
  <a href="https://arxiv.org/abs/2504.11713">Adjoint Sampling</a>,
  and <a href="https://arxiv.org/abs/2411.00698">Wasserstein Flow Matching</a>
  — see also <a href="https://bamos.github.io/presentations/2024/11/05/transport-between-distributions-over-distributions.html">these slides</a>)
</li>
<li>
  <b>amortization, meta-optimization, and meta-learning</b> between tasks
  (for
  <a href="https://arxiv.org/abs/2504.11713">sampling molecular conformers</a>,
  <a href="https://arxiv.org/abs/2206.05262">optimal transport</a>,
  <a href="https://arxiv.org/abs/2408.14608">flow matching</a>,
  <a href="https://arxiv.org/abs/2107.10254">convex optimization</a>,
  <a href="https://arxiv.org/abs/2404.16873">language model attacks</a>,
  and <a href="https://arxiv.org/abs/2309.07835">fixed point operations</a>
  — see also <a href="https://arxiv.org/abs/2202.00665">my tutorial</a>)
</li>
<li>
  <b>integrating structural information</b> and
  domain knowledge into AI systems to represent non-trivial reasoning operations,
  such as with <b>differentiable optimization</b> (in
  <a href="https://arxiv.org/abs/1703.00443">OptNet</a>,
  <a href="https://arxiv.org/abs/1609.07152">Input Convex Neural Networks</a>,
  <a href="https://arxiv.org/abs/1810.13400">End-to-End Task-Based Learning</a>,
  and <a href="https://arxiv.org/abs/1810.13400">Differentiable MPC</a>)
</li>
</ol>
<br>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
      <span class='cvdate'>2014&nbsp;-&nbsp;2019</span>
      <strong>Ph.D. in Computer Science</strong>, <em>Carnegie Mellon University</em>
        (0.00/0.00)
      <br>
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Thesis: <a href="https://github.com/bamos/thesis" target="_blank"><i>Differentiable Optimization-Based Modeling for Machine Learning</i></a>
        <br> Advisor: <a href="https://zicokolter.com" target="_blank">J. Zico Kolter</a>
        </p>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2011&nbsp;-&nbsp;2014</span>
      <strong>B.S. in Computer Science</strong>, <em>Virginia Tech</em>
        (3.99/4.00)
      <br>
    </td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Positions
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2019&nbsp;-&nbsp;2025</span>
<p markdown="1" style='margin: 0'><strong>Research Scientist</strong>, <em>Meta Superintelligence Labs</em>, New York City
</p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2024</span>
<p markdown="1" style='margin: 0'><strong>Visiting Lecturer</strong>, <em>Cornell Tech</em>, New York City
</p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2016&nbsp;-&nbsp;2019</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>Carnegie Mellon University</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://zicokolter.com" target="_blank">J. Zico Kolter</a> on ML and optimization)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2018</span>
<p markdown="1" style='margin: 0'><strong>Research Intern</strong>, <em>Intel Labs</em>, Santa Clara
<span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="http://vladlen.info/" target="_blank">Vladlen Koltun</a> on computer vision)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2017</span>
<p markdown="1" style='margin: 0'><strong>Research Intern</strong>, <em>Google DeepMind</em>, London
<span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://scholar.google.com/citations?user=nzEluBwAAAAJ" target="_blank">Nando de Freitas</a> and <a href="http://mdenil.com/" target="_blank">Misha Denil</a> on RL)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2014&nbsp;-&nbsp;2016</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>Carnegie Mellon University</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://www.cs.cmu.edu/~satya/" target="_blank">Mahadev Satyanarayanan</a> on mobile systems)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2014</span>
<p markdown="1" style='margin: 0'><strong>Research Intern</strong>, <em>Adobe Research</em>, San Jose
<span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://research.adobe.com/person/david-tompkins/" target="_blank">David Tompkins</a> on distributed systems)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2013&nbsp;-&nbsp;2014</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>Virginia Tech</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://people.cs.vt.edu/~ltw/shortvita.html" target="_blank">Layne Watson</a> and <a href="https://dblp.org/pid/75/8682.html" target="_blank">David Easterling</a> on optimization)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2012&nbsp;-&nbsp;2014</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>Virginia Tech</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://www.magnum.io/people/jules.html" target="_blank">Jules White</a> and <a href="https://scholar.google.com/citations?user=MRKab9cAAAAJ" target="_blank">Hamilton Turner</a> on mobile systems)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2012&nbsp;-&nbsp;2014</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>Virginia Tech</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(with <a href="https://www.ssrg.ece.vt.edu/" target="_blank">Binoy Ravindran</a> and <a href="https://scholar.google.com/citations?user=UG5yHRIAAAAJ" target="_blank">Alastair Murray</a> on compilers)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2013&nbsp;-&nbsp;2014</span>
<p markdown="1" style='margin: 0'><strong>Software Intern</strong>, <em>Snowplow</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(Scala development)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2013</span>
<p markdown="1" style='margin: 0'><strong>Software Intern</strong>, <em>Qualcomm</em>, San Diego
<span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(Python and C++ development)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2012</span>
<p markdown="1" style='margin: 0'><strong>Software Intern</strong>, <em>Phoenix Integration</em>, Virginia
<span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(C++, C\#, and Java development)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2011</span>
<p markdown="1" style='margin: 0'><strong>Network Administrator Intern</strong>, <em>Sunapsys</em>, Virginia
</p>
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Honors & Awards
<table class="table table-hover">
<tr>
  <td>
  <div style='float: right'>2025</div>
  <div>
        <a href="https://aistats.org/aistats2025/awards.html">AISTATS Best Reviewer</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2025</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2024</div>
  <div>
        <a href="https://icml.cc/virtual/2024/39245">Outstanding Paper Award at the ICML Theoretical Foundations Workshop</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2024</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2022</div>
  <div>
        <a href="https://neurips.cc/Conferences/2022/ProgramCommittee">NeurIPS Top Reviewer</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2022</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2022</div>
  <div>
        <a href="https://icml.cc/Conferences/2022/Reviewers">ICML Outstanding Reviewer</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2022</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2019</div>
  <div>
        <a href="https://iclr.cc/Conferences/2019/Awards">ICLR Outstanding Reviewer</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2019</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2017</div>
  <div>
        <a href="https://records.sigmm.org/2017/10/02/report-from-acm-mmsys-2017/">Best Paper Award at ACM MMSys</a>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2017</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2016&nbsp;-&nbsp;2019</div>
  <div>
        NSF Graduate Research Fellowship
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2016&nbsp;-&nbsp;2019</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2011&nbsp;-&nbsp;2014</div>
  <div>
        Nine undergraduate scholarships
    <br><p style="color:grey;font-size:1.2rem">Roanoke County Public Schools Engineering,
Salem-Roanoke County Chamber of Commerce,
Papa John's,
Scottish Rite of Freemasonry,
VT Intelligence Community Center for Academic Excellence,
VT Pamplin Leader,
VT Benjamin F. Bock, VT Gay B. Shober, VT I. Luck Gravett
</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2011&nbsp;-&nbsp;2014</td> -->
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Publications

<a href="https://scholar.google.com/citations?user=d8gdZR4AAAAJ" target="_blank">Google Scholar</a>: 11.4k+ citations and an h-index of 41. <br>
*Selected publications I am a primary author on are <span style='background-color: #ffffd0'>highlighted</span>.*

<h2>2025</h2>
<table class="table table-hover">

<tr id="tr-zheng2024onlineintrinsicrewardsdecision" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
1.
</td>
<td>

<em><a href='https://arxiv.org/abs/2410.23022' target='_blank'>Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zheng2024onlineintrinsicrewardsdecision").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/oni' target='_blank'>code</a>] <br>
<a href='https://enosair.github.io/' target='_blank'>Qinqing&nbsp;Zheng</a>, <a href='https://www.mikaelhenaff.com/' target='_blank'>Mikael&nbsp;Henaff</a>, <a href='https://amyzhang.github.io/' target='_blank'>Amy&nbsp;Zhang</a>, <a href='https://aditya-grover.github.io/' target='_blank'>Aditya&nbsp;Grover</a>, and <strong>Brandon&nbsp;Amos</strong><br>
RLC 2025  <br>

<div id="abs_zheng2024onlineintrinsicrewardsdecision" style="text-align: justify; display: none" markdown="1">
Automatically synthesizing dense rewards from natural language
descriptions is a promising paradigm in
reinforcement learning (RL), with applications to
sparse reward problems, open-ended exploration, and
hierarchical skill design. Recent works have made
promising steps by exploiting the prior knowledge of
large language models (LLMs). However, these
approaches suffer from important limitations: they
are either not scalable to problems requiring
billions of environment samples; or are limited to
reward functions expressible by compact code, which
may require source code and have difficulty
capturing nuanced semantics; or require a diverse
offline dataset, which may not exist or be
impossible to collect. In this work, we address
these limitations through a combination of
algorithmic and systems-level contributions. We
propose ONI, a distributed architecture that
simultaneously learns an RL policy and an intrinsic
reward function using LLM feedback. Our approach
annotates the agent's collected experience via an
asynchronous LLM server, which is then distilled
into an intrinsic reward model. We explore a range
of algorithmic choices for reward modeling with
varying complexity, including hashing, classification, and ranking models. By studying
their relative tradeoffs, we shed light on questions
regarding intrinsic reward design for sparse reward
problems. Our approach achieves state-of-the-art
performance across a range of challenging, sparse
reward tasks from the NetHack Learning Environment
in a simple unified process, solely using the
agent's gathered experience, without requiring
external datasets nor source code.
</div>

</td>
</tr>


<tr id="tr-paulus2024advprompter" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
2.
</td>
<td>

<em><a href='https://arxiv.org/abs/2404.16873' target='_blank'>AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_paulus2024advprompter").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/advprompter' target='_blank'>code</a>]  [<a href='https://bamos.github.io/presentations/2025/05/09/advprompter.html' target='_blank'>slides</a>] <br>
<a href='https://scholar.google.com/citations?user=njZL5CQAAAAJ' target='_blank'>Anselm&nbsp;Paulus*</a>, <a href='https://arman-z.github.io/' target='_blank'>Arman&nbsp;Zharmagambetov*</a>, <a href='https://sites.google.com/view/chuanguo' target='_blank'>Chuan&nbsp;Guo</a>, <strong>Brandon&nbsp;Amos<sup>&dagger;</sup></strong>, and <a href='https://yuandong-tian.com/' target='_blank'>Yuandong&nbsp;Tian<sup>&dagger;</sup></a><br>
ICML 2025  <br>

<div id="abs_paulus2024advprompter" style="text-align: justify; display: none" markdown="1">
While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming.
On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the target LLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, approximately 800 times faster than existing optimization-based approaches.
We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the target LLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the target LLM is lured to give a harmful response. Experimental results on popular open source target LLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by Advprompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.
</div>

</td>
</tr>


<tr id="tr-haviv2024wasserstein" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
3.
</td>
<td>
<a href='https://arxiv.org/abs/2411.00698' target='_blank'><img src="images/publications/haviv2024wasserstein.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2411.00698' target='_blank'>Wasserstein Flow Matching: Generative modeling over families of distributions</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_haviv2024wasserstein").toggle()'>abs</a>] [<a href='https://github.com/DoronHav/WassersteinFlowMatching' target='_blank'>code</a>] <br>
<a href='https://doronhav.github.io/' target='_blank'>Doron&nbsp;Haviv</a>, <a href='https://arampooladian.com/' target='_blank'>Aram-Alexandre&nbsp;Pooladian</a>, <a href='https://scholar.google.com/citations?user=aJOeGRoAAAAJ' target='_blank'>Dana&nbsp;Pe'er</a>, and <strong>Brandon&nbsp;Amos</strong><br>
ICML 2025  <br>

<div id="abs_haviv2024wasserstein" style="text-align: justify; display: none" markdown="1">
Generative modeling typically concerns the transport of a single
source distribution to a single target distribution
by learning (i.e., regressing onto) simple
probability flows. However, in modern data-driven
fields such as computer graphics and single-cell
genomics, samples (say, point-clouds) from datasets
can themselves be viewed as distributions (as, say, discrete measures). In these settings, the standard
generative modeling paradigm of flow matching would
ignore the relevant geometry of the samples. To
remedy this, we propose Wasserstein flow
matching (WFM), which appropriately lifts flow
matching onto families of distributions by appealing
to the Riemannian nature of the Wasserstein
geometry. Our algorithm leverages theoretical and
computational advances in (entropic) optimal
transport, as well as the attention mechanism in our
neural network architecture. We present two novel
algorithmic contributions. First, we demonstrate how
to perform generative modeling over Gaussian
distributions, where we generate representations of
granular cell states from single-cell genomics
data. Secondly, we show that WFM can learn flows
between high-dimensional and variable sized
point-clouds and synthesize cellular
microenvironments from spatial transcriptomics
datasets.
</div>

</td>
</tr>


<tr id="tr-havens2025adjointsamplinghighlyscalable" >
<td align='right' style='padding-left:0;padding-right:0;'>
4.
</td>
<td>

<em><a href='https://arxiv.org/abs/2504.11713' target='_blank'>Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_havens2025adjointsamplinghighlyscalable").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/adjoint_sampling' target='_blank'>code</a>] <br>
<a href='https://aaronhavens.github.io/' target='_blank'>Aaron&nbsp;Havens</a>, <a href='https://scholar.google.com/citations?user=IrCdg_wAAAAJ' target='_blank'>Benjamin&nbsp;Kurt&nbsp;Miller</a>, <a href='https://scholar.google.com/citations?user=337w9-V_WxIC' target='_blank'>Bing&nbsp;Yan</a>, <a href='https://cdenrich.github.io/' target='_blank'>Carles&nbsp;Domingo-Enrich</a>, <a href='https://scholar.google.com/citations?user=D4uRc_UAAAAJ' target='_blank'>Anuroop&nbsp;Sriram</a>, <a href='https://scholar.google.com/citations?user=KbqboRgAAAAJ' target='_blank'>Brandon&nbsp;Wood</a>, <a href='https://scholar.google.com/citations?user=AcE5Wt4AAAAJ' target='_blank'>Daniel&nbsp;Levine</a>, <a href='https://scholar.google.com/citations?user=o3tVp68AAAAJ' target='_blank'>Bin&nbsp;Hu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=Wewcpo4AAAAJ' target='_blank'>Brian&nbsp;Karrer</a>, <a href='https://scholar.google.com/citations?user=Cb-ZgHEAAAAJ' target='_blank'>Xiang&nbsp;Fu</a>, <a href='https://scholar.google.com/citations?user=2Dt0VJ4AAAAJ' target='_blank'>Guan-Horng&nbsp;Liu</a>, and <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a><br>
ICML 2025  <br>

<div id="abs_havens2025adjointsamplinghighlyscalable" style="text-align: justify; display: none" markdown="1">
We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.
</div>

</td>
</tr>


<tr id="tr-phan2024exactbytelevelprobabilitiestokenized" >
<td align='right' style='padding-left:0;padding-right:0;'>
5.
</td>
<td>

<em><a href='https://arxiv.org/abs/2410.09303' target='_blank'>Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_phan2024exactbytelevelprobabilitiestokenized").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/Exact-Byte-Level-Probabilities-from-Tokenized-LMs' target='_blank'>code</a>] <br>
<a href='https://truongbuu.github.io/' target='_blank'>Buu&nbsp;Phan</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://itaigat.com/' target='_blank'>Itai&nbsp;Gat</a>, <a href='https://mhavasi.github.io/' target='_blank'>Marton&nbsp;Havasi</a>, <a href='https://mmuckley.github.io/' target='_blank'>Matthew&nbsp;Muckley</a>, and <a href='https://karenullrich.info/' target='_blank'>Karen&nbsp;Ullrich</a><br>
ICLR 2025  <br>

<div id="abs_phan2024exactbytelevelprobabilitiestokenized" style="text-align: justify; display: none" markdown="1">
Tokenization is associated with many poorly understood shortcomings in language models (LMs), yet remains an important component for long sequence scaling purposes. This work studies how tokenization impacts model performance by analyzing and comparing the stochastic behavior of tokenized models with their byte-level, or token-free, counterparts. We discover that, even when the two models are statistically equivalent, their predictive distributions over the next byte can be substantially different, a phenomenon we term as "tokenization bias". To fully characterize this phenomenon, we introduce the Byte-Token Representation Lemma, a framework that establishes a mapping between the learned token distribution and its equivalent byte-level distribution. From this result, we develop a next-byte sampling algorithm that eliminates tokenization bias without requiring further training or optimization. In other words, this enables zero-shot conversion of tokenized LMs into statistically equivalent token-free ones. We demonstrate its broad applicability with two use cases: fill-in-the-middle (FIM) tasks and model ensembles. In FIM tasks where input prompts may terminate mid-token, leading to out-of-distribution tokenization, our method mitigates performance degradation and achieves an approximately 18% improvement in FIM coding benchmarks, consistently outperforming the standard token healing fix. For model ensembles where each model employs a distinct vocabulary, our approach enables seamless integration, resulting in improved performance (up to 3.7%) over individual models across various standard baselines in reasoning, knowledge, and coding.
</div>

</td>
</tr>


<tr id="tr-atanackovic2024meta" >
<td align='right' style='padding-left:0;padding-right:0;'>
6.
</td>
<td>
<a href='https://arxiv.org/abs/2408.14608' target='_blank'><img src="images/publications/atanackovic2024meta.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2408.14608' target='_blank'>Meta Flow Matching:  Integrating Vector Fields on the Wasserstein Manifold</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_atanackovic2024meta").toggle()'>abs</a>] [<a href='https://github.com/lazaratan/meta-flow-matching' target='_blank'>code</a>] <br>
<a href='https://lazaratan.github.io/' target='_blank'>Lazar&nbsp;Atanackovic</a>, <a href='https://scholar.google.com/citations?user=CblgXekAAAAJ' target='_blank'>Xi&nbsp;Zhang</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.cs.mcgill.ca/~blanchem/' target='_blank'>Mathieu&nbsp;Blanchette</a>, <a href='https://scholar.google.ca/citations?user=DN3LoTEAAAAJ' target='_blank'>Leo&nbsp;J&nbsp;Lee</a>, <a href='https://yoshuabengio.org/profile/' target='_blank'>Yoshua&nbsp;Bengio</a>, <a href='https://www.alextong.net/' target='_blank'>Alexander&nbsp;Tong</a>, and <a href='https://necludov.github.io/' target='_blank'>Kirill&nbsp;Neklyudov</a><br>
ICLR 2025  <br>

<div id="abs_atanackovic2024meta" style="text-align: justify; display: none" markdown="1">
Numerous biological and physical processes can be modeled as systems of interacting entities evolving continuously over time, e.g. the dynamics of communicating cells or physical particles. Learning the dynamics of such systems is essential for predicting the temporal evolution of populations across novel samples and unseen environments. Flow-based models allow for learning these dynamics at the population level - they model the evolution of the entire distribution of samples. However, current flow-based models are limited to a single initial population and a set of predefined conditions which describe different dynamics. We argue that multiple processes in natural sciences have to be represented as vector fields on the Wasserstein manifold of probability densities. That is, the change of the population at any moment in time depends on the population itself due to the interactions between samples. In particular, this is crucial for personalized medicine where the development of diseases and their respective treatment response depends on the microenvironment of cells specific to each patient. We propose Meta Flow Matching (MFM), a practical approach to integrating along these vector fields on the Wasserstein manifold by amortizing the flow model over the initial populations. Namely, we embed the population of samples using a Graph Neural Network (GNN) and use these embeddings to train a Flow Matching model. This gives MFM the ability to generalize over the initial distributions unlike previously proposed methods. We demonstrate the ability of MFM to improve prediction of individual treatment responses on a large scale multi-patient single-cell drug screen dataset.
</div>

</td>
</tr>


<tr id="tr-silvestri2024score" >
<td align='right' style='padding-left:0;padding-right:0;'>
7.
</td>
<td>

<em><a href='https://arxiv.org/abs/2307.05213' target='_blank'>Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_silvestri2024score").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=yHEb8eAAAAAJ' target='_blank'>Mattia&nbsp;Silvestri</a>, <a href='https://scholar.google.com/citations?user=sMtjmx4AAAAJ' target='_blank'>Senne&nbsp;Berden</a>, <a href='https://jayantamandi.com/' target='_blank'>Jayanta&nbsp;Mandi</a>, <a href='https://scholar.google.com/citations?user=muyZLrYAAAAJ' target='_blank'>Ali&nbsp;İrfan&nbsp;Mahmutoğulları</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://people.cs.kuleuven.be/~tias.guns/' target='_blank'>Tias&nbsp;Guns</a>, and <a href='https://scholar.google.com/citations?user=lJJ6EOMAAAAJ' target='_blank'>Michele&nbsp;Lombardi</a><br>
JAIR 2025  <br>

<div id="abs_silvestri2024score" style="text-align: justify; display: none" markdown="1">
Many real-world optimization problems contain parameters that are unknown before deployment time, either due to stochasticity or to lack of information (e.g., demand or travel times in delivery problems). A common strategy in such cases is to estimate said parameters via machine learning (ML) models trained to minimize the prediction error, which however is not necessarily aligned with the downstream task-level error. The decision-focused learning (DFL) paradigm overcomes this limitation by training to directly minimize a task loss, e.g. regret. Since the latter has non-informative gradients for combinatorial problems, state-of-the-art DFL methods introduce surrogates and approximations that enable training. But these methods exploit specific assumptions about the problem structures (e.g., convex or linear problems, unknown parameters only in the objective function). We propose an alternative method that makes no such assumptions, it combines stochastic smoothing with score function gradient estimation which works on any task loss. This opens up the use of DFL methods to nonlinear objectives, uncertain parameters in the problem constraints, and even two-stage stochastic optimization. Experiments show that it typically requires more epochs, but that it is on par with specialized methods and performs especially well for the difficult case of problems with uncertainty in the constraints, in terms of solution quality, scalability, or both.
</div>

</td>
</tr>


<tr id="tr-zhu2025advprefixobjectivenuancedllm" >
<td align='right' style='padding-left:0;padding-right:0;'>
8.
</td>
<td>

<em><a href='https://arxiv.org/abs/2412.10321' target='_blank'>AdvPrefix: An Objective for Nuanced LLM Jailbreaks</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zhu2025advprefixobjectivenuancedllm").toggle()'>abs</a>]<br>
<a href='https://schzhu.github.io/' target='_blank'>Sicheng&nbsp;Zhu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://yuandong-tian.com/' target='_blank'>Yuandong&nbsp;Tian</a>, <a href='https://sites.google.com/view/chuanguo' target='_blank'>Chuan&nbsp;Guo</a>, and <a href='https://ivanevtimov.eu/' target='_blank'>Ivan&nbsp;Evtimov</a><br>
NeurIPS 2025  <br>

<div id="abs_zhu2025advprefixobjectivenuancedllm" style="text-align: justify; display: none" markdown="1">
Many jailbreak attacks on large language models (LLMs) rely on a
common objective: making the model respond with the
prefix "Sure, here is (harmful request)". While
straightforward, this objective has two limitations:
limited control over model behaviors, often
resulting in incomplete or unrealistic responses, and a rigid format that hinders optimization. To
address these limitations, we introduce AdvPrefix, a
new prefix-forcing objective that enables more
nuanced control over model behavior while being easy
to optimize. Our objective leverages model-dependent
prefixes, automatically selected based on two
criteria: high prefilling attack success rates and
low negative log-likelihood. It can further simplify
optimization by using multiple prefixes for a single
user request. AdvPrefix can integrate seamlessly
into existing jailbreak attacks to improve their
performance for free. For example, simply replacing
GCG attack's target prefixes with ours on Llama-3
improves nuanced attack success rates from 14% to
80%, suggesting that current alignment struggles to
generalize to unseen prefixes. Our work demonstrates
the importance of jailbreak objectives in achieving
nuanced jailbreaks.
</div>

</td>
</tr>


<tr id="tr-press2025algotune" >
<td align='right' style='padding-left:0;padding-right:0;'>
9.
</td>
<td>

<em><a href='https://arxiv.org/abs/2507.15887' target='_blank'>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_press2025algotune").toggle()'>abs</a>]<br>
<a href='https://oripress.com/' target='_blank'>Ori&nbsp;Press</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://hyzhao.me/' target='_blank'>Haoyu&nbsp;Zhao</a>, <a href='https://yikai-wu.github.io/' target='_blank'>Yikai&nbsp;Wu</a>, <a href='https://samlikes.pizza/' target='_blank'>Samuel&nbsp;K.&nbsp;Ainsworth</a>, <a href='https://krupke.cc/' target='_blank'>Dominik&nbsp;Krupke</a>, <a href='https://kidger.site/' target='_blank'>Patrick&nbsp;Kidger</a>, <a href='https://www.linkedin.com/in/touqir-sajed-6a95b1126/' target='_blank'>Touqir&nbsp;Sajed</a>, <a href='https://stellato.io/' target='_blank'>Bartolomeo&nbsp;Stellato</a>, <a href='https://jisunp515.github.io/' target='_blank'>Jisun&nbsp;Park</a>, <a href='https://nathanaelbosch.github.io/' target='_blank'>Nathanael&nbsp;Bosch</a>, <a href='https://openreview.net/profile?id=~Eli_Meril1' target='_blank'>Eli&nbsp;Meril</a>, <a href='https://scholar.google.com/citations?user=_nbf9ZIAAAAJ' target='_blank'>Albert&nbsp;Steppi</a>, <a href='https://arman-z.github.io/' target='_blank'>Arman&nbsp;Zharmagambetov</a>, <a href='https://fangzhaoz.github.io/' target='_blank'>Fangzhao&nbsp;Zhang</a>, <a href='https://davidppineiro.com/' target='_blank'>David&nbsp;Pérez-Piñeiro</a>, <a href='https://scholar.google.it/citations?user=a5HqhwUAAAAJ' target='_blank'>Alberto&nbsp;Mercurio</a>, <a href='https://jennyzhanni.com/' target='_blank'>Ni&nbsp;Zhan</a>, <a href='https://scholar.google.com/citations?user=mdd52kMAAAAJ' target='_blank'>Talor&nbsp;Abramovich</a>, <a href='https://www.lieret.net/' target='_blank'>Kilian&nbsp;Lieret</a>, <a href='https://hanlin-zhang.com/' target='_blank'>Hanlin&nbsp;Zhang</a>, <a href='https://www.hbs.edu/faculty/Pages/profile.aspx?facId=1542499' target='_blank'>Shirley&nbsp;Huang</a>, <a href='https://scholar.google.com/citations?user=0z0fNxUAAAAJ' target='_blank'>Matthias&nbsp;Bethge</a>, and <a href='https://ofir.io/about' target='_blank'>Ofir&nbsp;Press</a><br>
NeurIPS Datasets and Benchmarks Track 2025  <br>

<div id="abs_press2025algotune" style="text-align: justify; display: none" markdown="1">
Despite progress in language model (LM) capabilities, evaluations
have thus far focused on models’ performance on
tasks that humans have previously solved, including
in programming (Jimenez et al., 2024) and
mathematics (Glazer et al., 2024). We therefore
propose testing models’ ability to design and
implement algorithms in an open-ended benchmark: We
task LMs with writing code that efficiently solves
computationally challenging problems in computer
science, physics, and mathematics. Our AlgoTune
benchmark consists of 155 coding tasks collected
from domain experts and a framework for validating
and timing LM-synthesized solution code, which is
compared to reference implementations from popular
opensource packages. In addition, we develop a
baseline LM agent, AlgoTuner, and evaluate its
performance across a suite of frontier
models. AlgoTuner achieves an average 1.76x speedup
against our reference solvers, which use libraries
such as SciPy, sk-learn and CVXPY. However, we find
that current models fail to discover algorithmic
innovations, instead preferring surface-level
optimizations. We hope that AlgoTune catalyzes the
development of LM agents exhibiting creative problem
solving beyond state-of-the-art human performance.
</div>

</td>
</tr>


<tr id="tr-zhang2025cultivating" >
<td align='right' style='padding-left:0;padding-right:0;'>
10.
</td>
<td>

<em><a href='https://arxiv.org/abs/2507.09650' target='_blank'>Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zhang2025cultivating").toggle()'>abs</a>]<br>
<a href='https://lhz1029.github.io/' target='_blank'>Lily&nbsp;H&nbsp;Zhang</a>, <a href='http://smithamilli.com/' target='_blank'>Smitha&nbsp;Milli</a>, <a href='https://scholar.google.com/citations?user=O924n4AAAAAJ' target='_blank'>Karen&nbsp;Long&nbsp;Jusko</a>, <a href='https://scholar.google.com/citations?user=URWZ--QAAAAJ' target='_blank'>Jonathan&nbsp;Smith</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=QDtbGtIAAAAJ' target='_blank'>Wassim&nbsp;Bouaziz</a>, <a href='https://openreview.net/profile?id=~Jack_Kussman1' target='_blank'>Jack&nbsp;Kussman</a>, <a href='https://scholar.google.com/citations?user=dEvt4JoAAAAJ' target='_blank'>Manon&nbsp;Revel</a>, <a href='https://scholar.google.com/citations?user=v2Wni-EAAAAJ' target='_blank'>Lisa&nbsp;Titus</a>, <a href='https://scholar.google.com/citations?user=IkQv0loAAAAJ' target='_blank'>Bhaktipriya&nbsp;Radharapu</a>, <a href='https://scholar.google.com/citations?user=ev8Ilx0AAAAJ' target='_blank'>Jane&nbsp;Yu</a>, <a href='https://www.linkedin.com/in/vidyasarma' target='_blank'>Vidya&nbsp;Sarma</a>, <a href='https://openreview.net/profile?id=~Kristopher_Rose1' target='_blank'>Kristopher&nbsp;Rose</a>, and <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a><br>
ICML MoFA Workshop 2025  <br>

<div id="abs_zhang2025cultivating" style="text-align: justify; display: none" markdown="1">
How can large language models (LLMs) serve users with varying preferences that may conflict across cultural, political, or other dimensions? To advance this challenge, this paper establishes four key results. First, we demonstrate, through a large-scale multilingual human study with representative samples from five countries (N=15,000), that humans exhibit significantly more variation in preferences than the responses of 21 state-of-the-art LLMs. Second, we show that existing methods for preference dataset collection are insufficient for learning the diversity of human preferences even along two of the most salient dimensions of variability in global values, due to the underlying homogeneity of candidate responses. Third, we argue that this motivates the need for negatively-correlated sampling when generating candidate sets, and we show that simple prompt-based techniques for doing so significantly enhance the performance of alignment methods in learning heterogeneous preferences. Fourth, based on this novel candidate sampling approach, we collect and open-source Community Alignment, the largest and most representative multilingual and multi-turn preference dataset to date, featuring almost 200,000 comparisons from annotators spanning five countries. We hope that the Community Alignment dataset will be a valuable resource for improving the effectiveness of LLMs for a diverse global population.
</div>

</td>
</tr>


<tr id="tr-lee2025banelexplorationposteriorsgenerative" >
<td align='right' style='padding-left:0;padding-right:0;'>
11.
</td>
<td>

<em><a href='https://arxiv.org/abs/2510.09596' target='_blank'>BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_lee2025banelexplorationposteriorsgenerative").toggle()'>abs</a>]<br>
<a href='https://sangyun884.github.io/about/' target='_blank'>Sangyun&nbsp;Lee</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://gfanti.github.io/' target='_blank'>Giulia&nbsp;Fanti</a><br>
arXiv 2025  <br>

<div id="abs_lee2025banelexplorationposteriorsgenerative" style="text-align: justify; display: none" markdown="1">
Today's generative models thrive with large amounts of supervised data and informative reward functions characterizing the quality of the generation. They work under the assumptions that the supervised data provides knowledge to pre-train the model, and the reward function provides dense information about how to further improve the generation quality and correctness. However, in the hardest instances of important problems, two problems arise: (1) the base generative model attains a near-zero reward signal, and (2) calls to the reward oracle are expensive. This setting poses a fundamentally different learning challenge than standard reward-based post-training. To address this, we propose BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the model using failed attempts only, while minimizing the number of reward evaluations (NREs). Our method is based on the idea that the problem of learning regularities underlying failures can be cast as another, in-loop generative modeling problem. We then leverage this model to assess whether new data resembles previously seen failures and steer the generation away from them. We show that BaNEL can improve model performance without observing a single successful sample on several sparse-reward tasks, outperforming existing novelty-bonus approaches by up to several orders of magnitude in success rate, while using fewer reward evaluations.
</div>

</td>
</tr>

</table>
<h2>2024</h2>
<table class="table table-hover">

<tr id="tr-pooladian2024neural" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
12.
</td>
<td>
<a href='https://arxiv.org/abs/2406.00288' target='_blank'><img src="images/publications/pooladian2024neural.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2406.00288' target='_blank'>Neural Optimal Transport with Lagrangian Costs</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pooladian2024neural").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/lagrangian-ot' target='_blank'>code</a>] <br>
<a href='https://arampooladian.com/' target='_blank'>Aram-Alexandre&nbsp;Pooladian</a>, <a href='https://cdenrich.github.io/' target='_blank'>Carles&nbsp;Domingo-Enrich</a>, <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, and <strong>Brandon&nbsp;Amos</strong><br>
UAI 2024  <br>

<div id="abs_pooladian2024neural" style="text-align: justify; display: none" markdown="1">
We investigate the optimal transport problem between probability measures when the underlying cost function is understood to satisfy a least action principle, also known as a Lagrangian cost. These generalizations are useful when connecting observations from a physical system, where the transport dynamics are influenced by the geometry of the system, such as obstacles, (e.g., incorporating barrier functions in the Lagrangian) and allows practitioners to incorporate a priori knowledge of the underlying system such as non-Euclidean geometries (e.g., paths must be circular). Our contributions are of computational interest, where we demonstrate the ability to efficiently compute geodesics and amortize spline-based paths, which has not been done before, even in low dimensional problems. Unlike prior work, we also output the resulting Lagrangian optimal transport map without requiring an ODE solver. We demonstrate the effectiveness of our formulation on low-dimensional examples taken from  prior work.
</div>

</td>
</tr>


<tr id="tr-sambharya2024learning" >
<td align='right' style='padding-left:0;padding-right:0;'>
13.
</td>
<td>

<em><a href='https://arxiv.org/abs/2309.07835' target='_blank'>Learning to Warm-Start Fixed-Point Optimization Algorithms</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_sambharya2024learning").toggle()'>abs</a>] [<a href='https://github.com/stellatogrp/l2ws' target='_blank'>code</a>] <br>
<a href='https://rajivsambharya.github.io/' target='_blank'>Rajiv&nbsp;Sambharya</a>, <a href='https://sites.google.com/view/georgina-hall' target='_blank'>Georgina&nbsp;Hall</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://stellato.io/' target='_blank'>Bartolomeo&nbsp;Stellato</a><br>
JMLR 2024  <br>

<div id="abs_sambharya2024learning" style="text-align: justify; display: none" markdown="1">
We introduce a machine-learning framework to warm-start fixed-point optimization algorithms. Our architecture consists of a neural network mapping problem parameters to warm starts, followed by a predefined number of fixed-point iterations. We propose two loss functions designed to either minimize the fixed-point residual or the distance to a ground truth solution. In this way, the neural network predicts warm starts with the end-to-end goal of minimizing the downstream loss. An important feature of our architecture is its flexibility, in that it can predict a warm start for fixed-point algorithms run for any number of steps, without being limited to the number of steps it has been trained on. We provide PAC-Bayes generalization bounds on unseen data for common classes of fixed-point operators: contractive, linearly convergent, and averaged. Applying this framework to well-known applications in control, statistics, and signal processing, we observe a significant reduction in the number of iterations and solution time required to solve these problems, through learned warm starts.
</div>

</td>
</tr>


<tr id="tr-lotfi2024unlocking" >
<td align='right' style='padding-left:0;padding-right:0;'>
14.
</td>
<td>

<em><a href='https://arxiv.org/abs/2407.18158' target='_blank'>Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_lotfi2024unlocking").toggle()'>abs</a>] [<a href='https://github.com/YilunKuang/token-bounds-for-llms' target='_blank'>code</a>] <br>
<a href='https://sanaelotfi.github.io/' target='_blank'>Sanae&nbsp;Lotfi</a>, <a href='https://yilunkuang.github.io/' target='_blank'>Yilun&nbsp;Kuang</a>, <a href='https://mfinzi.github.io/' target='_blank'>Marc&nbsp;Anton&nbsp;Finzi</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://goldblum.github.io/' target='_blank'>Micah&nbsp;Goldblum</a>, and <a href='https://cims.nyu.edu/~andrewgw/' target='_blank'>Andrew&nbsp;Gordon&nbsp;Wilson</a><br>
NeurIPS 2024  <br>

<div id="abs_lotfi2024unlocking" style="text-align: justify; display: none" markdown="1">
Large language models (LLMs) with billions of parameters excel at
predicting the next token in a sequence. Recent work
computes non-vacuous compression-based
generalization bounds for LLMs, but these bounds are
vacuous for large models at the billion-parameter
scale. Moreover, these bounds are obtained through
restrictive compression techniques, bounding
compressed models that generate low-quality
text. Additionally, the tightness of these existing
bounds depends on the number of IID documents in a
training set rather than the much larger number of
non-IID constituent tokens, leaving untapped
potential for tighter bounds. In this work, we
instead use properties of martingales to derive
generalization bounds that benefit from the vast
number of tokens in LLM training sets. Since a
dataset contains far more tokens than documents, our
generalization bounds not only tolerate but actually
benefit from far less restrictive compression
schemes. With Monarch matrices, Kronecker
factorizations, and post-training quantization, we
achieve non-vacuous generalization bounds for LLMs
as large as LLaMA2-70B. Unlike previous approaches, our work achieves the first non-vacuous bounds for
models that are deployed in practice and generate
high-quality text.
</div>

</td>
</tr>


<tr id="tr-domingoenrich2024stochastic" >
<td align='right' style='padding-left:0;padding-right:0;'>
15.
</td>
<td>

<em><a href='https://arxiv.org/abs/2312.02027' target='_blank'>Stochastic Optimal Control Matching</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_domingoenrich2024stochastic").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/soc-matching' target='_blank'>code</a>] <br>
<a href='https://cdenrich.github.io/' target='_blank'>Carles&nbsp;Domingo-Enrich</a>, <a href='https://scholar.google.com/citations?user=el5gT4AAAAAJ' target='_blank'>Jiequn&nbsp;Han</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://cims.nyu.edu/~bruna/' target='_blank'>Joan&nbsp;Bruna</a>, and <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a><br>
NeurIPS 2024  <br>

<div id="abs_domingoenrich2024stochastic" style="text-align: justify; display: none" markdown="1">
Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for four different control settings. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that is of independent interest, e.g., for generative modeling.
</div>

</td>
</tr>


<tr id="tr-ju2024to" >
<td align='right' style='padding-left:0;padding-right:0;'>
16.
</td>
<td>

<em><a href='https://arxiv.org/abs/2410.16456' target='_blank'>To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ju2024to").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=YW5jp5QAAAAJ' target='_blank'>Da&nbsp;JU</a>, <a href='https://scholar.google.com/citations?user=SjbhMQEAAAAJ' target='_blank'>Song&nbsp;Jiang</a>, <a href='https://scholar.google.com/citations?user=v1Frtb0AAAAJ' target='_blank'>Andrew&nbsp;Cohen</a>, <a href='https://openreview.net/profile?id=~Aaron_Foss1' target='_blank'>Aaron&nbsp;Foss</a>, <a href='https://scholar.google.com/citations?user=ncFsSKMAAAAJ' target='_blank'>Sasha&nbsp;Mitts</a>, <a href='https://arman-z.github.io/' target='_blank'>Arman&nbsp;Zharmagambetov</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=v_sIgawAAAAJ' target='_blank'>Xian&nbsp;Li</a>, <a href='https://scholar.google.com/citations?user=Y9BLeTAAAAAJ' target='_blank'>Justine&nbsp;T&nbsp;Kao</a>, <a href='https://scholar.google.com/citations?user=7-a1MKEAAAAJ' target='_blank'>Maryam&nbsp;Fazel-Zarandi</a>, and <a href='https://yuandong-tian.com/' target='_blank'>Yuandong&nbsp;Tian</a><br>
EMNLP Demo 2024  <br>

<div id="abs_ju2024to" style="text-align: justify; display: none" markdown="1">
Travel planning is a challenging and time-consuming task that aims to find an itinerary which satisfies multiple, interdependent constraints regarding flights, accommodations, attractions, and other travel arrangements. In this paper, we propose To the Globe (TTG), a real-time demo system that takes natural language requests from users, translates it to symbolic form via a fine-tuned Large Language Model, and produces optimal travel itineraries with Mixed Integer Linear Programming solvers. The overall system takes ~5 seconds to reply to the user request with guaranteed itineraries. To train TTG, we develop a synthetic data pipeline that generates user requests, flight and hotel information in symbolic form without human annotations, based on the statistics of real-world datasets, and fine-tune an LLM to translate NL user requests to their symbolic form, which is sent to the symbolic solver to compute optimal itineraries. Our NL-symbolic translation achieves ~91% exact match in a backtranslation metric (i.e., whether the estimated symbolic form of generated natural language matches the groundtruth), and its returned itineraries have a ratio of 0.979 compared to the optimal cost of the ground truth user request. When evaluated by users, TTG achieves consistently high Net Promoter Scores (NPS) of 35-40% on generated itinerary.
</div>

</td>
</tr>

</table>
<h2>2023</h2>
<table class="table table-hover">

<tr id="tr-amos2023tutorial" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
17.
</td>
<td>
<a href='https://arxiv.org/abs/2202.00665' target='_blank'><img src="images/publications/amos2023tutorial.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2202.00665' target='_blank'>Tutorial on amortized optimization</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2023tutorial").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/amortized-optimization-tutorial' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong><br>
Foundations and Trends in Machine Learning 2023  <br>

<div id="abs_amos2023tutorial" style="text-align: justify; display: none" markdown="1">
Optimization is a ubiquitous modeling tool and is often deployed
in settings which repeatedly solve similar instances
of the same problem. Amortized optimization methods
use learning to predict the solutions to problems in
these settings, exploiting the shared structure
between similar problem instances. These methods
have been crucial in variational inference and
reinforcement learning and are capable of solving
optimization problems many orders of magnitudes
times faster than traditional optimization methods
that do not use amortization. This tutorial presents
an introduction to the amortized optimization
foundations behind these advancements and overviews
their applications in variational inference, sparse
coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal
transport, and deep equilibrium networks.
</div>

</td>
</tr>


<tr id="tr-amos2023amortizing" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
18.
</td>
<td>
<a href='https://arxiv.org/abs/2210.12153' target='_blank'><img src="images/publications/amos2023amortizing.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2210.12153' target='_blank'>On amortizing convex conjugates for optimal transport</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2023amortizing").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/w2ot' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong><br>
ICLR 2023  <br>

<div id="abs_amos2023amortizing" style="text-align: justify; display: none" markdown="1">
This paper focuses on computing the convex conjugate operation that
arises when solving Euclidean Wasserstein-2 optimal
transport problems. This conjugation, which is also
referred to as the Legendre-Fenchel conjugate or
c-transform, is considered difficult to compute and
in practice, Wasserstein-2 methods are limited by
not being able to exactly conjugate the dual
potentials in continuous space. I show that
combining amortized approximations to the conjugate
with a solver for fine-tuning is computationally
easy. This combination significantly improves the
quality of transport maps learned for the
Wasserstein-2 benchmark by Korotin et al. (2021) and
is able to model many 2-dimensional couplings and
flows considered in the literature.
</div>

</td>
</tr>


<tr id="tr-sambharya2023l2a" >
<td align='right' style='padding-left:0;padding-right:0;'>
19.
</td>
<td>

<em><a href='https://arxiv.org/abs/2212.08260' target='_blank'>End-to-End Learning to Warm-Start for Real-Time Quadratic Optimization</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_sambharya2023l2a").toggle()'>abs</a>] [<a href='https://github.com/stellatogrp/l2ws' target='_blank'>code</a>] <br>
<a href='https://rajivsambharya.github.io/' target='_blank'>Rajiv&nbsp;Sambharya</a>, <a href='https://sites.google.com/view/georgina-hall' target='_blank'>Georgina&nbsp;Hall</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://stellato.io/' target='_blank'>Bartolomeo&nbsp;Stellato</a><br>
L4DC 2023  <br>

<div id="abs_sambharya2023l2a" style="text-align: justify; display: none" markdown="1">
First-order methods are widely used to solve convex quadratic programs
(QPs) in real-time applications because of their low
per-iteration cost. However, they can suffer from
slow convergence to accurate solutions. In this
paper, we present a framework which learns an
effective warm-start for a popular first-order
method in real-time applications, Douglas-Rachford
(DR) splitting, across a family of parametric
QPs. This framework consists of two modules: a
feedforward neural network block, which takes as
input the parameters of the QP and outputs a
warm-start, and a block which performs a fixed
number of iterations of DR splitting from this
warm-start and outputs a candidate solution. A key
feature of our framework is its ability to do
end-to-end learning as we differentiate through the
DR iterations. To illustrate the effectiveness of
our method, we provide generalization bounds (based
on Rademacher complexity) that improve with the
number of training problems and number of iterations
simultaneously. We further apply our method to three
real-time applications and observe that, by learning
good warm-starts, we are able to significantly
reduce the number of iterations required to obtain
high-quality solutions.
</div>

</td>
</tr>


<tr id="tr-amos2023meta" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
20.
</td>
<td>

<em><a href='https://arxiv.org/abs/2206.05262' target='_blank'>Meta Optimal Transport</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2023meta").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/meta-ot' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen</a>, <a href='https://giulslu.github.io/' target='_blank'>Giulia&nbsp;Luise</a>, and <a href='https://ievred.github.io' target='_blank'>Ievgen&nbsp;Redko</a><br>
ICML 2023  <br>

<div id="abs_amos2023meta" style="text-align: justify; display: none" markdown="1">
We study the use of amortized optimization to predict optimal
transport (OT) maps from the input measures, which
we call Meta OT. This helps repeatedly solve similar
OT problems between different measures by leveraging
the knowledge and information present from past
problems to rapidly predict and solve new
problems. Otherwise, standard methods ignore the
knowledge of the past solutions and suboptimally
re-solve each problem from scratch. Meta OT models
surpass the standard convergence rates of
log-Sinkhorn solvers in the discrete setting and
convex potentials in the continuous setting. We
improve the computational time of standard OT
solvers by multiple orders of magnitude in discrete
and continuous transport settings between images, spherical data, and color palettes.
</div>

</td>
</tr>


<tr id="tr-pooladian2023multisample" >
<td align='right' style='padding-left:0;padding-right:0;'>
21.
</td>
<td>

<em><a href='https://arxiv.org/abs/2304.14772' target='_blank'>Multisample Flow Matching: Straightening Flows with Minibatch Couplings</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pooladian2023multisample").toggle()'>abs</a>]<br>
<a href='https://arampooladian.com/' target='_blank'>Aram-Alexandre&nbsp;Pooladian</a>, <a href='https://helibenhamu.github.io/' target='_blank'>Heli&nbsp;Ben-Hamu</a>, <a href='https://cdenrich.github.io/' target='_blank'>Carles&nbsp;Domingo-Enrich</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.wisdom.weizmann.ac.il/~ylipman/' target='_blank'>Yaron&nbsp;Lipman</a>, and <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a><br>
ICML 2023  <br>

<div id="abs_pooladian2023multisample" style="text-align: justify; display: none" markdown="1">
Simulation-free methods for training continuous-time generative models
construct probability paths that go between noise
distributions and individual data samples. Recent
works, such as Flow Matching, derived paths that are
optimal for each data sample. However, these
algorithms rely on independent data and noise
samples, and do not exploit underlying structure in
the data distribution for constructing probability
paths. We propose Multisample Flow Matching, a more
general framework that uses non-trivial couplings
between data and noise samples while satisfying the
correct marginal constraints. At very small overhead
costs, this generalization allows us to (i) reduce
gradient variance during training, (ii) obtain
straighter flows for the learned vector field, which
allows us to generate high-quality samples using
fewer function evaluations, and (iii) obtain
transport maps with lower cost in high dimensions, which has applications beyond generative
modeling. Importantly, we do so in a completely
simulation-free manner with a simple minimization
objective. We show that our proposed methods improve
sample consistency on downsampled ImageNet data
sets, and lead to better low-cost sample generation.
</div>

</td>
</tr>


<tr id="tr-zheng2023semi" >
<td align='right' style='padding-left:0;padding-right:0;'>
22.
</td>
<td>

<em><a href='https://arxiv.org/abs/2210.06518' target='_blank'>Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zheng2023semi").toggle()'>abs</a>]<br>
<a href='https://enosair.github.io/' target='_blank'>Qinqing&nbsp;Zheng</a>, <a href='https://www.mikaelhenaff.com/' target='_blank'>Mikael&nbsp;Henaff</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://aditya-grover.github.io/' target='_blank'>Aditya&nbsp;Grover</a><br>
ICML 2023  <br>

<div id="abs_zheng2023semi" style="text-align: justify; display: none" markdown="1">
Natural agents can effectively learn from multiple data sources that
differ in size, quality, and types of
measurements. We study this heterogeneity in the
context of offline reinforcement learning (RL) by
introducing a new, practically motivated
semi-supervised setting. Here, an agent has access
to two sets of trajectories: labelled trajectories
containing state, action, reward triplets at every
timestep, along with unlabelled trajectories that
contain only state and reward information. For this
setting, we develop a simple meta-algorithmic
pipeline that learns an inverse-dynamics model on
the labelled data to obtain proxy-labels for the
unlabelled data, followed by the use of any offline
RL algorithm on the true and proxy-labelled
trajectories. Empirically, we find this simple
pipeline to be highly successful&nbsp;-&nbsp;on several D4RL
benchmarks, certain offline RL
algorithms can match the performance of variants
trained on a fully labeled dataset even when we
label only 10% trajectories from the low return
regime. Finally, we perform a large-scale controlled
empirical study investigating the interplay of
data-centric properties of the labelled and
unlabelled datasets, with algorithmic design choices
(e.g., inverse dynamics, offline RL algorithm) to
identify general trends and best practices for
training RL agents on semi-supervised offline
datasets.
</div>

</td>
</tr>


<tr id="tr-bansal2023taskmet" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
23.
</td>
<td>

<em><a href='https://arxiv.org/abs/2312.05250' target='_blank'>TaskMet: Task-Driven Metric Learning for Model Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_bansal2023taskmet").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/taskmet' target='_blank'>code</a>] <br>
<a href='https://dishank-b.github.io/' target='_blank'>Dishank&nbsp;Bansal</a>, <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, <a href='https://www.mustafamukadam.com/' target='_blank'>Mustafa&nbsp;Mukadam</a>, and <strong>Brandon&nbsp;Amos</strong><br>
NeurIPS 2023  <br>

<div id="abs_bansal2023taskmet" style="text-align: justify; display: none" markdown="1">
Deep learning models are often used with some downstream
task. Models solely trained to achieve accurate
predictions may struggle to perform well on
the desired downstream tasks. We propose using the
task's loss to learn a metric which parameterizes a
loss to train the model.This approach does not alter
the optimal prediction model itself, but rather
changes the model learning to emphasize the
information important for the downstream task.This
enables us to achieve the best of both worlds:a
prediction model trained in the original prediction
space while also being valuable for the desired
downstream task.We validate our approach through
experiments conducted in two main settings: 1)
decision-focused model learning scenarios involving
portfolio optimization and budget allocation, and2)
reinforcement learning in noisy environments with
distracting states.
</div>

</td>
</tr>


<tr id="tr-zharmagambetov2023landscape" >
<td align='right' style='padding-left:0;padding-right:0;'>
24.
</td>
<td>

<em><a href='https://arxiv.org/abs/2307.08964' target='_blank'>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zharmagambetov2023landscape").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/LANCER' target='_blank'>code</a>] <br>
<a href='https://arman-z.github.io/' target='_blank'>Arman&nbsp;Zharmagambetov</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=TuVq07oAAAAJ' target='_blank'>Aaron&nbsp;Ferber</a>, <a href='https://taoanhuang.github.io/' target='_blank'>Taoan&nbsp;Huang</a>, <a href='https://scholar.google.com/citations?user=1jjyaBYAAAAJ' target='_blank'>Bistra&nbsp;Dilkina</a>, and <a href='https://yuandong-tian.com/' target='_blank'>Yuandong&nbsp;Tian</a><br>
NeurIPS 2023  <br>

<div id="abs_zharmagambetov2023landscape" style="text-align: justify; display: none" markdown="1">
Recent works in learning-integrated optimization have shown promise in
settings where the optimization problem is only
partially observed or where general-purpose
optimizers perform poorly without expert tuning. By
learning an optimizer g to tackle these challenging
problems with f as the objective, the optimization
process can be substantially accelerated by
leveraging past experience. Training the optimizer
can be done with supervision from known optimal
solutions (not always available) or implicitly by
optimizing the compound function f ∘ g , but the
implicit approach is slow and challenging due to
frequent calls to the optimizer and sparse
gradients, particularly for combinatorial
solvers. To address these challenges, we propose
using a smooth and learnable Landscape Surrogate
M instead of composing f with g . This surrogate can be computed
faster than g, provides dense and smooth gradients
during training, can generalize to unseen
optimization problems, and is efficiently learned
via alternating optimization. We test our approach
on both synthetic problems and real-world problems, achieving comparable or superior objective values
compared to state-of-the-art baselines while
reducing the number of calls to g . Notably, our
approach outperforms existing methods for
computationally expensive high-dimensional problems.
</div>

</td>
</tr>


<tr id="tr-retchin2023koopman" >
<td align='right' style='padding-left:0;padding-right:0;'>
25.
</td>
<td>

<em><a href='https://openreview.net/pdf?id=3W7vPqWCeM' target='_blank'>Koopman Constrained Policy Optimization: A Koopman operator theoretic method for differentiable optimal control in robotics</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_retchin2023koopman").toggle()'>abs</a>]<br>
<a href='https://www.linkedin.com/in/matthew-retchin/' target='_blank'>Matthew&nbsp;Retchin</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.eigensteve.com/' target='_blank'>Steven&nbsp;Brunton</a>, and <a href='https://shurans.github.io/' target='_blank'>Shuran&nbsp;Song</a><br>
ICML Differentiable Almost Everything Workshop 2023  <br>

<div id="abs_retchin2023koopman" style="text-align: justify; display: none" markdown="1">
We introduce Koopman Constrained Policy Optimization (KCPO), combining implicitly differentiable model predictive
control with a deep Koopman autoencoder for robot
learning in unknown and nonlinear dynamical
systems. KCPO is a new policy optimization algorithm
that trains neural policies end-to-end with hard box
constraints on controls. Guaranteed satisfaction of
hard constraints helps ensure the performance and
safety of robots. We perform imitation learning with
KCPO to recover expert policies on the Simple
Pendulum, Cartpole Swing-Up, Reacher, and
Differential Drive environments, outperforming
baseline methods in generalizing to
out-of-distribution constraints in most environments
after training.
</div>

</td>
</tr>

</table>
<h2>2022</h2>
<table class="table table-hover">

<tr id="tr-fickinger2021crossdomain" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
26.
</td>
<td>
<a href='https://arxiv.org/abs/2110.03684' target='_blank'><img src="images/publications/fickinger2021crossdomain.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2110.03684' target='_blank'>Cross-Domain Imitation Learning via Optimal Transport</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_fickinger2021crossdomain").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/gwil' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=bBFN_qwAAAAJ' target='_blank'>Arnaud&nbsp;Fickinger</a>, <a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen</a>, <a href='http://people.eecs.berkeley.edu/~russell/' target='_blank'>Stuart&nbsp;Russell</a>, and <strong>Brandon&nbsp;Amos</strong><br>
ICLR 2022  <br>

<div id="abs_fickinger2021crossdomain" style="text-align: justify; display: none" markdown="1">
Cross-domain imitation learning studies how to leverage expert
demonstrations of one agent to train an imitation
agent with a different embodiment or
morphology. Comparing trajectories and stationary
distributions between the expert and imitation
agents is challenging because they live on different
systems that may not even have the same
dimensionality. We propose Gromov-Wasserstein
Imitation Learning (GWIL), a method for cross-domain
imitation that uses the Gromov-Wasserstein distance
to align and compare states between the different
spaces of the agents. Our theory formally
characterizes the scenarios where GWIL preserves
optimality, revealing its possibilities and
limitations. We demonstrate the effectiveness of
GWIL in non-trivial continuous control domains
ranging from simple rigid transformation of the
expert domain to arbitrary transformation of the
state-action space.
</div>

</td>
</tr>


<tr id="tr-benhamu2022matching" >
<td align='right' style='padding-left:0;padding-right:0;'>
27.
</td>
<td>

<em><a href='https://arxiv.org/abs/2207.04711' target='_blank'>Matching Normalizing Flows and Probability Paths on Manifolds</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_benhamu2022matching").toggle()'>abs</a>]<br>
<a href='https://helibenhamu.github.io/' target='_blank'>Heli&nbsp;Ben-Hamu*</a>, <a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen*</a>, <a href='https://joeybose.github.io/' target='_blank'>Joey&nbsp;Bose</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://aditya-grover.github.io/' target='_blank'>Aditya&nbsp;Grover</a>, <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a>, <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, and <a href='https://www.wisdom.weizmann.ac.il/~ylipman/' target='_blank'>Yaron&nbsp;Lipman</a><br>
ICML 2022  <br>

<div id="abs_benhamu2022matching" style="text-align: justify; display: none" markdown="1">
Continuous Normalizing Flows (CNFs) are a class of generative models
that transform a prior distribution to a model
distribution by solving an ordinary differential
equation (ODE). We propose to train CNFs on
manifolds by minimizing probability path divergence
(PPD), a novel family of divergences between the
probability density path generated by the CNF and a
target probability density path. PPD is formulated
using a logarithmic mass conservation formula which
is a linear first order partial differential
equation relating the log target probabilities and
the CNF's defining vector field. PPD has several key
benefits over existing methods: it sidesteps the
need to solve an ODE per iteration, readily applies
to manifold data, scales to high dimensions, and is
compatible with a large family of target paths
interpolating pure noise and data in finite
time. Theoretically, PPD is shown to bound classical
probability divergences. Empirically, we show that
CNFs learned by minimizing PPD achieve
state-of-the-art results in likelihoods and sample
quality on existing low-dimensional manifold
benchmarks, and is the first example of a generative
model to scale to moderately high dimensional
manifolds.
</div>

</td>
</tr>


<tr id="tr-chen2022semi" >
<td align='right' style='padding-left:0;padding-right:0;'>
28.
</td>
<td>
<a href='https://arxiv.org/abs/2203.06832' target='_blank'><img src="images/publications/chen2022semi.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2203.06832' target='_blank'>Semi-Discrete Normalizing Flows through Differentiable Tessellation</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2022semi").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a><br>
NeurIPS 2022  <br>

<div id="abs_chen2022semi" style="text-align: justify; display: none" markdown="1">
Mapping between discrete and continuous distributions is a difficult
task and many have had to resort to approximate or
heuristical approaches. We propose a
tessellation-based approach that directly learns
quantization boundaries on a continuous space, complete with exact likelihood evaluations. This is
done through constructing normalizing flows on
convex polytopes parameterized through a
differentiable Voronoi tessellation. Using a simple
homeomorphism with an efficient log determinant
Jacobian, we can then cheaply parameterize
distributions on convex polytopes.

We explore this approach in two application settings, mapping from
discrete to continuous and vice versa. Firstly, a
Voronoi dequantization allows automatically learning
quantization boundaries in a multidimensional
space. The location of boundaries and distances
between regions can encode useful structural
relations between the quantized discrete
values. Secondly, a Voronoi mixture model has
constant computation cost for likelihood evaluation
regardless of the number of mixture
components. Empirically, we show improvements over
existing methods across a range of structured data
modalities, and find that we can achieve a
significant gain from just adding Voronoi mixtures
to a baseline model.
</div>

</td>
</tr>


<tr id="tr-pineda2022theseus" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
29.
</td>
<td>

<em><a href='https://arxiv.org/abs/2207.09442' target='_blank'>Theseus: A Library for Differentiable Nonlinear Optimization</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pineda2022theseus").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/theseus' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=rebEn8oAAAAJ' target='_blank'>Luis&nbsp;Pineda</a>, <a href='https://scholar.google.com/citations?user=3PJeg1wAAAAJ' target='_blank'>Taosha&nbsp;Fan</a>, <a href='https://scholar.google.com/citations?user=gpgb4LgAAAAJ' target='_blank'>Maurizio&nbsp;Monge</a>, <a href='https://scholar.google.com/citations?user=BFWurDEAAAAJ' target='_blank'>Shobha&nbsp;Venkataraman</a>, <a href='https://psodhi.github.io/' target='_blank'>Paloma&nbsp;Sodhi</a>, <a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;Chen</a>, <a href='https://joeaortiz.github.io/' target='_blank'>Joseph&nbsp;Ortiz</a>, <a href='https://danieldetone.com/' target='_blank'>Daniel&nbsp;DeTone</a>, <a href='https://scholar.google.com/citations?user=keDqjK0AAAAJ' target='_blank'>Austin&nbsp;Wang</a>, <a href='https://scholar.google.com/citations?user=8orqBsYAAAAJ' target='_blank'>Stuart&nbsp;Anderson</a>, <a href='https://www.linkedin.com/in/jing-dong-24b26ab3/' target='_blank'>Jing&nbsp;Dong</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://www.mustafamukadam.com/' target='_blank'>Mustafa&nbsp;Mukadam</a><br>
NeurIPS 2022  <br>

<div id="abs_pineda2022theseus" style="text-align: justify; display: none" markdown="1">
We present Theseus, an efficient application-agnostic open source
library for differentiable nonlinear least squares
(DNLS) optimization built on PyTorch, providing a
common framework for end-to-end structured learning
in robotics and vision. Existing DNLS
implementations are application specific and do not
always incorporate many ingredients important for
efficiency. Theseus is application-agnostic, as we
illustrate with several example applications that
are built using the same underlying differentiable
components, such as second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse
solvers, automatic vectorization, batching, GPU
acceleration, and gradient computation with implicit
differentiation and direct loss minimization. We do
extensive performance evaluation in a set of
applications, demonstrating significant efficiency
gains and better scalability when these features are
incorporated.
</div>

</td>
</tr>


<tr id="tr-vinitsky2022nocturne" >
<td align='right' style='padding-left:0;padding-right:0;'>
30.
</td>
<td>

<em><a href='https://arxiv.org/abs/2206.09889' target='_blank'>Nocturne: a driving benchmark for multi-agent learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_vinitsky2022nocturne").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/nocturne' target='_blank'>code</a>] <br>
<a href='https://www.eugenevinitsky.com' target='_blank'>Eugene&nbsp;Vinitsky</a>, <a href='https://www.nathanlct.com/about' target='_blank'>Nathan&nbsp;Lichtlé</a>, <a href='https://www.linkedin.com/in/xiaomeng-yang-356a976b/' target='_blank'>Xiaomeng&nbsp;Yang</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://www.jakobfoerster.com/' target='_blank'>Jakob&nbsp;Foerster</a><br>
NeurIPS Datasets and Benchmarks Track 2022  <br>

<div id="abs_vinitsky2022nocturne" style="text-align: justify; display: none" markdown="1">
We introduce Nocturne, a new 2D driving simulator for
investigating multi-agent coordination under partial
observability. The focus of Nocturne is to enable
research into inference and theory of mind in
real-world multi-agent settings without the
computational overhead of computer vision and
feature extraction from images. Agents in this
simulator only observe an obstructed view of the
scene, mimicking human visual sensing
constraints. Unlike existing benchmarks that are
bottlenecked by rendering human-like observations
directly using a camera input, Nocturne uses
efficient intersection methods to compute a
vectorized set of visible features in a C++
back-end, allowing the simulator to run at 2000+
steps-per-second. Using open-source trajectory and
map data, we construct a simulator to load and
replay arbitrary trajectories and scenes from
real-world driving data. Using this environment, we
benchmark reinforcement-learning and
imitation-learning agents and demonstrate that the
agents are quite far from human-level coordination
ability and deviate significantly from the expert
trajectories.
</div>

</td>
</tr>

</table>
<h2>2021</h2>
<table class="table table-hover">

<tr id="tr-amos2021modelbased" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
31.
</td>
<td>
<a href='https://arxiv.org/abs/2008.12775' target='_blank'><img src="images/publications/amos2021modelbased.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2008.12775' target='_blank'>On the model-based stochastic value gradient for continuous reinforcement learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2021modelbased").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/svg' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2021.svg.pdf' target='_blank'>slides</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://samuelstanton.github.io/' target='_blank'>Samuel&nbsp;Stanton</a>, <a href='https://cs.nyu.edu/~dy1042/' target='_blank'>Denis&nbsp;Yarats</a>, and <a href='https://cims.nyu.edu/~andrewgw/' target='_blank'>Andrew&nbsp;Gordon&nbsp;Wilson</a><br>
L4DC 2021 (Oral) <br>

<div id="abs_amos2021modelbased" style="text-align: justify; display: none" markdown="1">
Model-based reinforcement learning approaches add explicit domain
knowledge to agents in hopes of improving the
sample-efficiency in comparison to model-free
agents. However, in practice model-based methods are
unable to achieve the same asymptotic performance on
challenging continuous control tasks due to the
complexity of learning and controlling an explicit
world model. In this paper we investigate the
stochastic value gradient (SVG), which is a
well-known family of methods for controlling
continuous systems which includes model-based
approaches that distill a model-based value
expansion into a model-free policy. We consider a
variant of the model-based SVG that scales to larger
systems and uses 1) an entropy regularization to
help with exploration, 2) a learned deterministic
world model to improve the short-horizon value
estimate, and 3) a learned model-free value estimate
after the model's rollout. This SVG variation
captures the model-free soft actor-critic method as
an instance when the model rollout horizon is zero, and otherwise uses short-horizon model rollouts to
improve the value estimate for the policy update. We
surpass the asymptotic performance of other
model-based methods on the proprioceptive MuJoCo
locomotion tasks from the OpenAI gym, including a
humanoid. We notably achieve these results with a
simple deterministic world model without requiring
an ensemble.
</div>

</td>
</tr>


<tr id="tr-cohen2021riemannian" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
32.
</td>
<td>
<a href='https://arxiv.org/abs/2106.10272' target='_blank'><img src="images/publications/cohen2021riemannian.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2106.10272' target='_blank'>Riemannian Convex Potential Maps</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_cohen2021riemannian").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/rcpm' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2021.rcpm.pdf' target='_blank'>slides</a>] <br>
<a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen*</a>, <strong>Brandon&nbsp;Amos*</strong>, and <a href='https://www.wisdom.weizmann.ac.il/~ylipman/' target='_blank'>Yaron&nbsp;Lipman</a><br>
ICML 2021  <br>

<div id="abs_cohen2021riemannian" style="text-align: justify; display: none" markdown="1">
Modeling distributions on Riemannian manifolds is a crucial
component in understanding non-Euclidean data that
arises, e.g., in physics and geology. The budding
approaches in this space are limited by
representational and computational tradeoffs. We
propose and study a class of flows that uses convex
potentials from Riemannian optimal transport. These
are universal and can model distributions on any
compact Riemannian manifold without requiring domain
knowledge of the manifold to be integrated into the
architecture. We demonstrate that these flows can
model standard distributions on spheres, and tori, on synthetic and geological data.
</div>

</td>
</tr>


<tr id="tr-paulus2021comboptnet" >
<td align='right' style='padding-left:0;padding-right:0;'>
33.
</td>
<td>
<a href='https://arxiv.org/abs/2105.02343' target='_blank'><img src="images/publications/paulus2021comboptnet.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2105.02343' target='_blank'>CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_paulus2021comboptnet").toggle()'>abs</a>] [<a href='https://github.com/martius-lab/CombOptNet' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=njZL5CQAAAAJ' target='_blank'>Anselm&nbsp;Paulus</a>, <a href='https://mrolinek.github.io/' target='_blank'>Michal&nbsp;Rol&iacute;nek</a>, <a href='https://scholar.google.com/citations?user=hA1rlU4AAAAJ' target='_blank'>V&iacute;t&nbsp;Musil</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://al.is.mpg.de/person/gmartius' target='_blank'>Georg&nbsp;Martius</a><br>
ICML 2021  <br>

<div id="abs_paulus2021comboptnet" style="text-align: justify; display: none" markdown="1">
Bridging logical and algorithmic reasoning with modern machine
learning techniques is a fundamental challenge with
potentially transformative impact. On the
algorithmic side, many NP-hard problems can be
expressed as integer programs, in which the
constraints play the role of their "combinatorial
specification". In this work, we aim to integrate
integer programming solvers into neural network
architectures as layers capable of learning both the
cost terms and the constraints. The resulting
end-to-end trainable architectures jointly extract
features from raw data and solve a suitable
(learned) combinatorial problem with
state-of-the-art integer programming solvers. We
demonstrate the potential of such layers with an
extensive performance analysis on synthetic data and
with a demonstration on a competitive computer
vision keypoint matching benchmark.
</div>

</td>
</tr>


<tr id="tr-fickinger2021scalable" >
<td align='right' style='padding-left:0;padding-right:0;'>
34.
</td>
<td>

<em><a href='https://arxiv.org/abs/2109.15316' target='_blank'>Scalable Online Planning via Reinforcement Learning Fine-Tuning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_fickinger2021scalable").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=bBFN_qwAAAAJ' target='_blank'>Arnaud&nbsp;Fickinger</a>, <a href='https://scholar.google.com/citations?user=sJwwn54AAAAJ' target='_blank'>Hengyuan&nbsp;Hu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='http://people.eecs.berkeley.edu/~russell/' target='_blank'>Stuart&nbsp;Russell</a>, and <a href='https://noambrown.github.io/' target='_blank'>Noam&nbsp;Brown</a><br>
NeurIPS 2021  <br>

<div id="abs_fickinger2021scalable" style="text-align: justify; display: none" markdown="1">
Lookahead search has been a critical component of recent AI successes, such as in the games of chess, go, and
poker. However, the search methods used in these
games, and in many other settings, are
tabular. Tabular search methods do not scale well
with the size of the search space, and this problem
is exacerbated by stochasticity and partial
observability. In this work we replace tabular
search with online model-based fine-tuning of a
policy neural network via reinforcement learning, and show that this approach outperforms
state-of-the-art search algorithms in benchmark
settings. In particular, we use our search algorithm
to achieve a new state-of-the-art result in
self-play Hanabi, and show the generality of our
algorithm by also showing that it outperforms
tabular search in the Atari game Ms. Pacman.
</div>

</td>
</tr>


<tr id="tr-cohen2020aligning" >
<td align='right' style='padding-left:0;padding-right:0;'>
35.
</td>
<td>
<a href='https://arxiv.org/abs/2006.12648' target='_blank'><img src="images/publications/cohen2020aligning.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2006.12648' target='_blank'>Aligning Time Series on Incomparable Spaces</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_cohen2020aligning").toggle()'>abs</a>] [<a href='https://github.com/samcohen16/Aligning-Time-Series' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2021.gdtw.pdf' target='_blank'>slides</a>] <br>
<a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen</a>, <a href='https://giulslu.github.io/' target='_blank'>Giulia&nbsp;Luise</a>, <a href='https://avt.im/' target='_blank'>Alexander&nbsp;Terenin</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://www.deisenroth.cc/' target='_blank'>Marc&nbsp;Peter&nbsp;Deisenroth</a><br>
AISTATS 2021  <br>

<div id="abs_cohen2020aligning" style="text-align: justify; display: none" markdown="1">
Dynamic time warping (DTW) is a useful method for aligning, comparing
and combining time series, but it requires them to
live in comparable spaces. In this work, we consider
a setting in which time series live on different
spaces without a sensible ground metric, causing DTW
to become ill-defined. To alleviate this, we propose
Gromov dynamic time warping (GDTW), a distance
between time series on potentially incomparable
spaces that avoids the comparability requirement by
instead considering intra-relational geometry. We
derive a Frank-Wolfe algorithm for computing it and
demonstrate its effectiveness at aligning, combining
and comparing time series living on incomparable
spaces. We further propose a smoothed version of
GDTW as a differentiable loss and assess its
properties in a variety of settings, including
barycentric averaging, generative modeling and
imitation learning.
</div>

</td>
</tr>


<tr id="tr-chen2021learning" >
<td align='right' style='padding-left:0;padding-right:0;'>
36.
</td>
<td>
<a href='https://arxiv.org/abs/2011.03902' target='_blank'><img src="images/publications/chen2021learning.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2011.03902' target='_blank'>Learning Neural Event Functions for Ordinary Differential Equations</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2021learning").toggle()'>abs</a>] [<a href='https://github.com/rtqichen/torchdiffeq' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a><br>
ICLR 2021  <br>

<div id="abs_chen2021learning" style="text-align: justify; display: none" markdown="1">
The existing Neural ODE formulation relies on an explicit
knowledge of the termination time. We extend Neural
ODEs to implicitly defined termination criteria
modeled by neural event functions, which can be
chained together and differentiated through. Neural
Event ODEs are capable of modeling discrete
(instantaneous) changes in a continuous-time system, without prior knowledge of when these changes should
occur or how many such changes should exist. We test
our approach in modeling hybrid discrete- and
continuous- systems such as switching dynamical
systems and collision in multi-body systems, and we
propose simulation-based training of point processes
with applications in discrete control.
</div>

</td>
</tr>


<tr id="tr-chen2021neural" >
<td align='right' style='padding-left:0;padding-right:0;'>
37.
</td>
<td>
<a href='https://arxiv.org/abs/2011.04583' target='_blank'><img src="images/publications/chen2021neural.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2011.04583' target='_blank'>Neural Spatio-Temporal Point Processes</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2021neural").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/neural_stpp' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=7MxQd6UAAAAJ' target='_blank'>Ricky&nbsp;T.&nbsp;Q.&nbsp;Chen</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a><br>
ICLR 2021  <br>

<div id="abs_chen2021neural" style="text-align: justify; display: none" markdown="1">
We propose a new class of parameterizations for spatio-temporal
point processes which leverage Neural ODEs as a
computational method and enable flexible, high-fidelity models of discrete events that are
localized in continuous time and space. Central to
our approach is a combination of recurrent
continuous-time neural networks with two novel
neural architectures, i.e., Jump and Attentive
Continuous-time Normalizing Flows. This approach
allows us to learn complex distributions for both
the spatial and temporal domain and to condition
non-trivially on the observed event history. We
validate our models on data sets from a wide variety
of contexts such as seismology, epidemiology, urban
mobility, and neuroscience.
</div>

</td>
</tr>


<tr id="tr-yarats2021improving" >
<td align='right' style='padding-left:0;padding-right:0;'>
38.
</td>
<td>

<em><a href='https://arxiv.org/abs/1910.01741' target='_blank'>Improving Sample Efficiency in Model-Free Reinforcement Learning from Images</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_yarats2021improving").toggle()'>abs</a>] [<a href='https://sites.google.com/view/sac-ae' target='_blank'>code</a>] <br>
<a href='https://cs.nyu.edu/~dy1042/' target='_blank'>Denis&nbsp;Yarats</a>, <a href='https://amyzhang.github.io/' target='_blank'>Amy&nbsp;Zhang</a>, <a href='https://scholar.google.com/citations?user=PTS2AOgAAAAJ' target='_blank'>Ilya&nbsp;Kostrikov</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.cs.mcgill.ca/~jpineau/' target='_blank'>Joelle&nbsp;Pineau</a>, and <a href='https://scholar.google.com/citations?user=GgQ9GEkAAAAJ&h' target='_blank'>Rob&nbsp;Fergus</a><br>
AAAI 2021  <br>

<div id="abs_yarats2021improving" style="text-align: justify; display: none" markdown="1">
Training an agent to solve control tasks directly from
high-dimensional images with model-free
reinforcement learning (RL) has proven
difficult. The agent needs to learn a latent
representation together with a control policy to
perform the task. Fitting a high-capacity encoder
using a scarce reward signal is not only sample
inefficient, but also prone to suboptimal
convergence. Two ways to improve sample efficiency
are to extract relevant features for the task and
use off-policy algorithms. We dissect various
approaches of learning good latent features, and
conclude that the image reconstruction loss is the
essential ingredient that enables efficient and
stable representation learning in image-based
RL. Following these findings, we devise an
off-policy actor-critic algorithm with an auxiliary
decoder that trains end-to-end and matches
state-of-the-art performance across both model-free
and model-based algorithms on many challenging
control tasks. We release our code to encourage
future research on image-based RL.
</div>

</td>
</tr>


<tr id="tr-venkataraman2021neural" >
<td align='right' style='padding-left:0;padding-right:0;'>
39.
</td>
<td>

<em><a href='https://arxiv.org/abs/2107.10254' target='_blank'>Neural Fixed-Point Acceleration for Convex Optimization</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_venkataraman2021neural").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/neural-scs' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=BFWurDEAAAAJ' target='_blank'>Shobha&nbsp;Venkataraman*</a> and <strong>Brandon&nbsp;Amos*</strong><br>
ICML AutoML Workshop 2021  <br>

<div id="abs_venkataraman2021neural" style="text-align: justify; display: none" markdown="1">
Fixed-point iterations are at the heart of numerical computing and
are often a computational bottleneck in real-time
applications that typically need a fast solution of
moderate accuracy. We present neural fixed-point
acceleration which combines ideas from meta-learning
and classical acceleration methods to automatically
learn to accelerate fixed-point problems that are
drawn from a distribution. We apply our framework to
SCS, the state-of-the-art solver for convex cone
programming, and design models and loss functions to
overcome the challenges of learning over unrolled
optimization and acceleration instabilities. Our
work brings neural acceleration into any
optimization problem expressible with CVXPY.
</div>

</td>
</tr>


<tr id="tr-cohen2021sliced" >
<td align='right' style='padding-left:0;padding-right:0;'>
40.
</td>
<td>

<em><a href='https://arxiv.org/abs/2102.07115' target='_blank'>Sliced Multi-Marginal Optimal Transport</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_cohen2021sliced").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen</a>, <a href='https://avt.im/' target='_blank'>Alexander&nbsp;Terenin</a>, <a href='https://scholar.google.com/citations?user=jmM-JlIAAAAJ' target='_blank'>Yannik&nbsp;Pitcan</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.deisenroth.cc/' target='_blank'>Marc&nbsp;Peter&nbsp;Deisenroth</a>, and <a href='https://scholar.google.co.in/citations?user=FPVUA-YAAAAJ' target='_blank'>K&nbsp;S&nbsp;Sesh&nbsp;Kumar</a><br>
NeurIPS OTML Workshop 2021  <br>

<div id="abs_cohen2021sliced" style="text-align: justify; display: none" markdown="1">
Multi-marginal optimal transport enables one to compare multiple
probability measures, which increasingly finds
application in multi-task learning problems. One
practical limitation of multi-marginal transport is
computational scalability in the number of measures, samples and dimensionality. In this work, we propose
a multi-marginal optimal transport paradigm based on
random one-dimensional projections, whose
(generalized) distance we term the sliced
multi-marginal Wasserstein distance. To construct
this distance, we introduce a characterization of
the one-dimensional multi-marginal Kantorovich
problem and use it to highlight a number of
properties of the sliced multi-marginal Wasserstein
distance. In particular, we show that (i) the sliced
multi-marginal Wasserstein distance is a
(generalized) metric that induces the same topology
as the standard Wasserstein distance, (ii) it admits
a dimension-free sample complexity, (iii) it is
tightly connected with the problem of barycentric
averaging under the sliced-Wasserstein metric. We
conclude by illustrating the sliced multi-marginal
Wasserstein on multi-task density estimation and
multi-dynamics reinforcement learning problems.
</div>

</td>
</tr>


<tr id="tr-richterpowell2021input" >
<td align='right' style='padding-left:0;padding-right:0;'>
41.
</td>
<td>

<em><a href='https://arxiv.org/abs/2111.12187' target='_blank'>Input Convex Gradient Networks</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_richterpowell2021input").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=L78pVMMAAAAJ' target='_blank'>Jack&nbsp;Richter-Powell</a>, <a href='https://scholar.google.com/citations?user=Hzf8bu0AAAAJ' target='_blank'>Jonathan&nbsp;Lorraine</a>, and <strong>Brandon&nbsp;Amos</strong><br>
NeurIPS OTML Workshop 2021  <br>

<div id="abs_richterpowell2021input" style="text-align: justify; display: none" markdown="1">
The gradients of convex functions are expressive models of non-trivial
vector fields. For example, Brenier's theorem yields
that the optimal transport map between any two
measures on Euclidean space under the squared
distance is realized as a convex gradient, which is
a key insight used in recent generative flow
models. In this paper, we study how to model convex
gradients by integrating a Jacobian-vector product
parameterized by a neural network, which we call the
Input Convex Gradient Network (ICGN). We
theoretically study ICGNs and compare them to taking
the gradient of an Input-Convex Neural Network
(ICNN), empirically demonstrating that a single
layer ICGN can fit a toy example better than a
single layer ICNN. Lastly, we explore extensions to
deeper networks and connections to constructions
from Riemannian geometry.
</div>

</td>
</tr>


<tr id="tr-cohen2021imitation" >
<td align='right' style='padding-left:0;padding-right:0;'>
42.
</td>
<td>

<em><a href='https://openreview.net/pdf?id=Xe5MFhFvYGX' target='_blank'>Imitation Learning from Pixel Observations for Continuous Control</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_cohen2021imitation").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=CmdjfTsAAAAJ' target='_blank'>Samuel&nbsp;Cohen</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.deisenroth.cc/' target='_blank'>Marc&nbsp;Peter&nbsp;Deisenroth</a>, <a href='https://www.mikaelhenaff.com/' target='_blank'>Mikael&nbsp;Henaff</a>, <a href='https://www.eugenevinitsky.com' target='_blank'>Eugene&nbsp;Vinitsky</a>, and <a href='https://cs.nyu.edu/~dy1042/' target='_blank'>Denis&nbsp;Yarats</a><br>
NeurIPS DeepRL Workshop 2021  <br>

<div id="abs_cohen2021imitation" style="text-align: justify; display: none" markdown="1">
We study imitation learning from visual observations only for
controlling dynamical systems with continuous states
and actions. This setting is attractive due to the
large amount of video data available from which
agents could learn from. However, it is challenging
due to i) not observing the actions and ii) the
high-dimensional visual space. In this setting, we
explore recipes for imitation learning based on
adversarial learning and optimal transport. These
recipes enable us to scale these methods to attain
expert-level performance on visual continuous
control tasks in the DeepMind control suite. We
investigate the tradeoffs of these approaches and
present a comprehensive evaluation of the key design
choices. To encourage reproducible research in this
area, we provide an easy-to-use implementation for
benchmarking visual imitation learning, including
our methods and expert demonstrations.
</div>

</td>
</tr>


<tr id="tr-pineda2021mbrl" >
<td align='right' style='padding-left:0;padding-right:0;'>
43.
</td>
<td>

<em><a href='https://arxiv.org/abs/2104.10159' target='_blank'>MBRL-Lib: A Modular Library for Model-based Reinforcement Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pineda2021mbrl").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/mbrl-lib' target='_blank'>code</a>] <br>
<a href='https://scholar.google.com/citations?user=rebEn8oAAAAJ' target='_blank'>Luis&nbsp;Pineda</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://amyzhang.github.io/' target='_blank'>Amy&nbsp;Zhang</a>, <a href='https://www.natolambert.com/' target='_blank'>Nathan&nbsp;Lambert</a>, and <a href='https://www.robertocalandra.com/about/' target='_blank'>Roberto&nbsp;Calandra</a><br>
arXiv 2021  <br>

<div id="abs_pineda2021mbrl" style="text-align: justify; display: none" markdown="1">
Model-based reinforcement learning is a compelling framework for
data-efficient learning of agents that interact with
the world. This family of algorithms has many
subcomponents that need to be carefully selected and
tuned. As a result the entry-bar for researchers to
approach the field and to deploy it in real-world
tasks can be daunting. In this paper, we present
MBRL-Lib&nbsp;-&nbsp;a machine learning library for
model-based reinforcement learning in continuous
state-action spaces based on PyTorch. MBRL-Lib is
designed as a platform for both researchers, to
easily develop, debug and compare new algorithms, and non-expert user, to lower the entry-bar of
deploying state-of-the-art algorithms.
</div>

</td>
</tr>

</table>
<h2>2020</h2>
<table class="table table-hover">

<tr id="tr-amos2020differentiable" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
44.
</td>
<td>

<em><a href='https://arxiv.org/abs/1909.12830' target='_blank'>The Differentiable Cross-Entropy Method</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2020differentiable").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/dcem' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2020.dcem.pdf' target='_blank'>slides</a>] <br>
<strong>Brandon&nbsp;Amos</strong> and <a href='https://cs.nyu.edu/~dy1042/' target='_blank'>Denis&nbsp;Yarats</a><br>
ICML 2020  <br>

<div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1">
We study the Cross-Entropy Method (CEM) for the non-convex
optimization of a continuous and parameterized
objective function and introduce a differentiable
variant (DCEM) that enables us to differentiate the
output of CEM with respect to the objective
function's parameters. In the machine learning
setting this brings CEM inside of the end-to-end
learning pipeline where this has otherwise been
impossible. We show applications in a synthetic
energy-based structured prediction task and in
non-convex continuous control. In the control
setting we show on the simulated cheetah and walker
tasks that we can embed their optimal action
sequences with DCEM and then use policy optimization
to fine-tune components of the controller as a step
towards combining model-based and model-free RL.
</div>

</td>
</tr>


<tr id="tr-lambert2020objective" >
<td align='right' style='padding-left:0;padding-right:0;'>
45.
</td>
<td>

<em><a href='https://arxiv.org/abs/2002.04523' target='_blank'>Objective Mismatch in Model-based Reinforcement Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_lambert2020objective").toggle()'>abs</a>]<br>
<a href='https://www.natolambert.com/' target='_blank'>Nathan&nbsp;Lambert</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=zSsW478AAAAJ' target='_blank'>Omry&nbsp;Yadan</a>, and <a href='https://www.robertocalandra.com/about/' target='_blank'>Roberto&nbsp;Calandra</a><br>
L4DC 2020  <br>

<div id="abs_lambert2020objective" style="text-align: justify; display: none" markdown="1">
Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework-what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model with respect to the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.
</div>

</td>
</tr>


<tr id="tr-amos2020QNSTOP" >
<td align='right' style='padding-left:0;padding-right:0;'>
46.
</td>
<td>

<em><a href='https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf' target='_blank'>QNSTOP: Quasi-Newton Algorithm for Stochastic Optimization</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2020QNSTOP").toggle()'>abs</a>] [<a href='https://github.com/vtopt/qnstop' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://dblp.org/pid/75/8682.html' target='_blank'>David&nbsp;Easterling</a>, <a href='https://people.cs.vt.edu/~ltw/shortvita.html' target='_blank'>Layne&nbsp;T.&nbsp;Watson</a>, <a href='https://scholar.google.com/citations?user=2I6IgikAAAAJ' target='_blank'>William&nbsp;Thacker</a>, <a href='https://dblp.org/pid/142/1258.html' target='_blank'>Brent&nbsp;Castle</a>, and <a href='https://mtrosset.pages.iu.edu/' target='_blank'>Michael&nbsp;Trosset</a><br>
ACM TOMS 2020  <br>

<div id="abs_amos2020QNSTOP" style="text-align: justify; display: none" markdown="1">
QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the
quasi-Newton stochastic optimization method of Castle and Trosset. For
stochastic problems, convergence theory exists for the particular
algorithmic choices and parameter values used in QNSTOP. Both the parallel
driver subroutine, which offers several parallel decomposition strategies, and the serial driver subroutine can be used for stochastic optimization or
deterministic global optimization, based on an input switch. QNSTOP is
particularly effective for “noisy” deterministic problems, using only
objective function values. Some performance data for computational systems
biology problems is given.
</div>

</td>
</tr>


<tr id="tr-sercu2020neural" >
<td align='right' style='padding-left:0;padding-right:0;'>
47.
</td>
<td>
<a href='https://www.biorxiv.org/content/10.1101/2021.04.08.439084v1.abstract' target='_blank'><img src="images/publications/sercu2020neural.png" class="publicationImg" /></a> 
<em><a href='https://www.biorxiv.org/content/10.1101/2021.04.08.439084v1.abstract' target='_blank'>Neural Potts Model</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_sercu2020neural").toggle()'>abs</a>]<br>
<a href='https://tom.sercu.me/' target='_blank'>Tom&nbsp;Sercu</a>, <a href='https://dblp.org/pid/296/8930.html' target='_blank'>Robert&nbsp;Verkuil</a>, <a href='https://scholar.google.com/citations?user=2M0OltAAAAAJ' target='_blank'>Joshua&nbsp;Meier</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=ZDjmMuwAAAAJ' target='_blank'>Zeming&nbsp;Lin</a>, <a href='https://www.linkedin.com/in/caroline-chen/' target='_blank'>Caroline&nbsp;Chen</a>, <a href='https://www.linkedin.com/in/jasonliu6/' target='_blank'>Jason&nbsp;Liu</a>, <a href='http://yann.lecun.com/' target='_blank'>Yann&nbsp;LeCun</a>, and <a href='https://scholar.google.com/citations?user=vqb78-gAAAAJ' target='_blank'>Alexander&nbsp;Rives</a><br>
MLCB 2020  <br>

<div id="abs_sercu2020neural" style="text-align: justify; display: none" markdown="1">
We propose the Neural Potts Model objective as an amortized
optimization problem. The objective enables training
a single model with shared parameters to explicitly
model energy landscapes across multiple protein
families. Given a protein sequence as input, the
model is trained to predict a pairwise coupling
matrix for a Potts model energy function describing
the local evolutionary landscape of the
sequence. Couplings can be predicted for novel
sequences. A controlled ablation experiment
assessing unsupervised contact prediction on sets of
related protein families finds a gain from
amortization for low-depth multiple sequence
alignments; the result is then confirmed on a
database with broad coverage of protein sequences.
</div>

</td>
</tr>


<tr id="tr-lou2020riemannian" >
<td align='right' style='padding-left:0;padding-right:0;'>
48.
</td>
<td>
<a href='https://drive.google.com/file/d/1Ewro0Ne1tvK15nHyYopY4wZ59QTVB-1c/view' target='_blank'><img src="images/publications/lou2020riemannian.png" class="publicationImg" /></a> 
<em><a href='https://drive.google.com/file/d/1Ewro0Ne1tvK15nHyYopY4wZ59QTVB-1c/view' target='_blank'>Deep Riemannian Manifold Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_lou2020riemannian").toggle()'>abs</a>]<br>
<a href='https://aaronlou.com/' target='_blank'>Aaron&nbsp;Lou</a>, <a href='https://maxn.io/' target='_blank'>Maximilian&nbsp;Nickel</a>, and <strong>Brandon&nbsp;Amos</strong><br>
NeurIPS Geo4dl Workshop 2020  <br>

<div id="abs_lou2020riemannian" style="text-align: justify; display: none" markdown="1">
We present a new class of learnable Riemannian manifolds with a metric
parameterized by a deep neural network. The core manifold operations–specifically
the Riemannian exponential and logarithmic maps–are solved using approximate
numerical techniques. Input and parameter gradients are computed with an
adjoint sensitivity analysis. This enables us to fit geodesics and distances with
gradient-based optimization of both on-manifold values and the manifold itself.
We demonstrate our method’s capability to model smooth, flexible metric structures
in graph embedding tasks.
</div>

</td>
</tr>

</table>
<h2>2019</h2>
<table class="table table-hover">

<tr id="tr-amos2019differentiable" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
49.
</td>
<td>

<em><a href='https://github.com/bamos/thesis/raw/master/bamos_thesis.pdf' target='_blank'>Differentiable Optimization-Based Modeling for Machine Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2019differentiable").toggle()'>abs</a>] [<a href='https://github.com/bamos/thesis' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong><br>
Ph.D. Thesis 2019  <br>

<div id="abs_amos2019differentiable" style="text-align: justify; display: none" markdown="1">
Domain-specific modeling priors and specialized components are becoming
increasingly important to the machine learning field. These components integrate specialized knowledge that we have as humans into model. We argue in
this thesis that optimization methods provide an expressive set of operations
that should be part of the machine learning practitioner’s modeling toolbox.
We present two foundational approaches for optimization-based modeling:
1) the OptNet architecture that integrates optimization problems as individual
layers in larger end-to-end trainable deep networks, and 2) the input-convex
neural network (ICNN) architecture that helps make inference and learning in
deep energy-based models and structured prediction more tractable.
We then show how to use the OptNet approach 1) as a way of combining
model-free and model-based reinforcement learning and 2) for top-k learning
problems. We conclude by showing how to differentiate cone programs and turn
the cvxpy domain specific language into a differentiable optimization layer that
enables rapid prototyping of the approaches in this thesis.
</div>

</td>
</tr>


<tr id="tr-amos2019differentiable3" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
50.
</td>
<td>

<em><a href='http://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf' target='_blank'>Differentiable Convex Optimization Layers</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2019differentiable3").toggle()'>abs</a>] [<a href='https://github.com/cvxgrp/cvxpylayers' target='_blank'>code</a>] <br>
<a href='https://www.akshayagrawal.com/' target='_blank'>Akshay&nbsp;Agrawal*</a>, <strong>Brandon&nbsp;Amos*</strong>, <a href='https://scholar.google.com/citations?user=HmCZLyoAAAAJ' target='_blank'>Shane&nbsp;Barratt*</a>, <a href='https://web.stanford.edu/~boyd/' target='_blank'>Stephen&nbsp;Boyd*</a>, <a href='https://stevendiamond.me/' target='_blank'>Steven&nbsp;Diamond*</a>, and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter*</a><br>
NeurIPS 2019  <br>

<div id="abs_amos2019differentiable3" style="text-align: justify; display: none" markdown="1">
Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.
</div>

</td>
</tr>


<tr id="tr-amos2019limited" >
<td align='right' style='padding-left:0;padding-right:0;'>
51.
</td>
<td>
<a href='https://arxiv.org/abs/1906.08707' target='_blank'><img src="images/publications/amos2019limited.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/1906.08707' target='_blank'>The Limited Multi-Label Projection Layer</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2019limited").toggle()'>abs</a>] [<a href='https://github.com/locuslab/lml' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='http://vladlen.info/' target='_blank'>Vladlen&nbsp;Koltun</a>, and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter</a><br>
arXiv 2019  <br>

<div id="abs_amos2019limited" style="text-align: justify; display: none" markdown="1">
We propose the Limited Multi-Label (LML) projection layer as a new
primitive operation for end-to-end learning systems. The LML layer
provides a probabilistic way of modeling multi-label predictions
limited to having exactly k labels. We derive efficient forward and
backward passes for this layer and show how the layer can be used to
optimize the top-k recall for multi-label tasks with incomplete label
information. We evaluate LML layers on top-k CIFAR-100 classification
and scene graph generation. We show that LML layers add a negligible
amount of computational overhead, strictly improve the model's
representational capacity, and improve accuracy. We also revisit the
truncated top-k entropy method as a competitive baseline for top-k
classification.
</div>

</td>
</tr>


<tr id="tr-grefenstette2019generalized" >
<td align='right' style='padding-left:0;padding-right:0;'>
52.
</td>
<td>

<em><a href='https://arxiv.org/abs/1910.01727' target='_blank'>Generalized Inner Loop Meta-Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_grefenstette2019generalized").toggle()'>abs</a>] [<a href='https://github.com/facebookresearch/higher' target='_blank'>code</a>] <br>
<a href='https://www.egrefen.com/' target='_blank'>Edward&nbsp;Grefenstette</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://cs.nyu.edu/~dy1042/' target='_blank'>Denis&nbsp;Yarats</a>, <a href='https://phumonhtut.me/' target='_blank'>Phu&nbsp;Mon&nbsp;Htut</a>, <a href='https://amolchanov86.github.io/' target='_blank'>Artem&nbsp;Molchanov</a>, <a href='https://fmeier.github.io/' target='_blank'>Franziska&nbsp;Meier</a>, <a href='https://douwekiela.github.io/' target='_blank'>Douwe&nbsp;Kiela</a>, <a href='https://kyunghyuncho.me/' target='_blank'>Kyunghyun&nbsp;Cho</a>, and <a href='https://soumith.ch/' target='_blank'>Soumith&nbsp;Chintala</a><br>
arXiv 2019  <br>

<div id="abs_grefenstette2019generalized" style="text-align: justify; display: none" markdown="1">
Many (but not all) approaches self-qualifying as "meta-learning" in
deep learning and reinforcement learning fit a
common pattern of approximating the solution to a
nested optimization problem. In this paper, we give
a formalization of this shared pattern, which we
call GIMLI, prove its general requirements, and
derive a general-purpose algorithm for implementing
similar approaches. Based on this analysis and
algorithm, we describe a library of our design, higher, which we share with the community to assist
and enable future research into these kinds of
meta-learning approaches. We end the paper by
showcasing the practical applications of this
framework and library through illustrative
experiments and ablation studies which they
facilitate.
</div>

</td>
</tr>

</table>
<h2>2018</h2>
<table class="table table-hover">

<tr id="tr-amos2018learning" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
53.
</td>
<td>
<a href='https://arxiv.org/abs/1804.06318' target='_blank'><img src="images/publications/amos2018learning.png" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/1804.06318' target='_blank'>Learning Awareness Models</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2018learning").toggle()'>abs</a>]<br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://laurent-dinh.github.io/' target='_blank'>Laurent&nbsp;Dinh</a>, <a href='https://scholar.google.com/citations?user=l-HhJaUAAAAJ' target='_blank'>Serkan&nbsp;Cabi</a>, <a href='https://dblp.org/pid/188/6045.html' target='_blank'>Thomas&nbsp;Roth&ouml;rl</a>, <a href='https://scholar.google.com/citations?user=0Dkf68EAAAAJ' target='_blank'>Sergio&nbsp;G&oacute;mez&nbsp;Colmenarejo</a>, <a href='https://scholar.google.com/citations?user=YfgdfyYAAAAJ' target='_blank'>Alistair&nbsp;Muldal</a>, <a href='https://scholar.google.com/citations?user=gVFnjOcAAAAJ' target='_blank'>Tom&nbsp;Erez</a>, <a href='https://scholar.google.com/citations?user=CjOTm_4AAAAJ' target='_blank'>Yuval&nbsp;Tassa</a>, <a href='https://scholar.google.com/citations?user=nzEluBwAAAAJ' target='_blank'>Nando&nbsp;de&nbsp;Freitas</a>, and <a href='https://mdenil.com/' target='_blank'>Misha&nbsp;Denil</a><br>
ICLR 2018  <br>

<div id="abs_amos2018learning" style="text-align: justify; display: none" markdown="1">
We consider the setting of an agent with a fixed body interacting with an
unknown and uncertain external world. We show that models
trained to predict proprioceptive information about the
agent's body come to represent objects in the external world.
In spite of being trained with only internally available
signals, these dynamic body models come to represent external
objects through the necessity of predicting their effects on
the agent's own body. That is, the model learns holistic
persistent representations of objects in the world, even
though the only training signals are body signals. Our
dynamics model is able to successfully predict distributions
over 132 sensor readings over 100 steps into the future and we
demonstrate that even when the body is no longer in contact
with an object, the latent variables of the dynamics model
continue to represent its shape. We show that active data
collection by maximizing the entropy of predictions about the
body-touch sensors, proprioception and vestibular
information-leads to learning of dynamic models that show
superior performance when used for control. We also collect
data from a real robotic hand and show that the same models
can be used to answer questions about properties of objects in
the real world. Videos with qualitative results of our models
are available <a href="https://goo.gl/mZuqAV">here</a>.
</div>

</td>
</tr>


<tr id="tr-amos2018end" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
54.
</td>
<td>

<em><a href='https://arxiv.org/abs/1810.13400' target='_blank'>Differentiable MPC for End-to-end Planning and Control</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2018end").toggle()'>abs</a>] [<a href='https://locuslab.github.io/mpc.pytorch/' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://ivandariojr.io/' target='_blank'>Ivan&nbsp;Dario&nbsp;Jimenez&nbsp;Rodriguez</a>, <a href='https://scholar.google.com/citations?user=Th4PuGkAAAAJ' target='_blank'>Jacob&nbsp;Sacks</a>, <a href='https://homes.cs.washington.edu/~bboots/' target='_blank'>Byron&nbsp;Boots</a>, and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter</a><br>
NeurIPS 2018  <br>

<div id="abs_amos2018end" style="text-align: justify; display: none" markdown="1">
In this paper we present foundations for using model predictive control (MPC) as a differentiable policy class in reinforcement learning. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the solver. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning in a larger system. We empirically show results in an imitation learning setting, demonstrating that we can recover the underlying dynamics and cost more efficiently and reliably than with a generic neural network policy class
</div>

</td>
</tr>


<tr id="tr-brown2018depth" >
<td align='right' style='padding-left:0;padding-right:0;'>
55.
</td>
<td>

<em><a href='http://arxiv.org/abs/1805.08195' target='_blank'>Depth-Limited Solving for Imperfect-Information Games</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_brown2018depth").toggle()'>abs</a>]<br>
<a href='https://noambrown.github.io/' target='_blank'>Noam&nbsp;Brown</a>, <a href='http://www.cs.cmu.edu/~sandholm/' target='_blank'>Tuomas&nbsp;Sandholm</a>, and <strong>Brandon&nbsp;Amos</strong><br>
NeurIPS 2018  <br>

<div id="abs_brown2018depth" style="text-align: justify; display: none" markdown="1">
A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.
</div>

</td>
</tr>


<tr id="tr-wang2018enabling" >
<td align='right' style='padding-left:0;padding-right:0;'>
56.
</td>
<td>

<em><a href='https://dl.acm.org/citation.cfm?id=3209659' target='_blank'>Enabling Live Video Analytics with a Scalable and Privacy-Aware Framework</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_wang2018enabling").toggle()'>abs</a>]<br>
<a href='https://www.junjuewang.com' target='_blank'>Junjue&nbsp;Wang</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://anupamdas.org/' target='_blank'>Anupam&nbsp;Das</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='https://www.normsadeh.org/' target='_blank'>Norman&nbsp;Sadeh</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
ACM TOMM 2018  <br>

<div id="abs_wang2018enabling" style="text-align: justify; display: none" markdown="1">
We show how to build the components of a privacy-aware, live video
analytics ecosystem from the bottom up, starting
with OpenFace, our new open-source face recognition
system that approaches state-of-the-art
accuracy. Integrating OpenFace with interframe
tracking, we build RTFace, a mechanism for
denaturing video streams that selectively blurs
faces according to specified policies at full frame
rates. This enables privacy management for live
video analytics while providing a secure approach
for handling retrospective policy
exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks
using RTFace and show how it can be an enabler for a
vibrant ecosystem and marketplace of privacy-aware
video streams and analytics services.
</div>

</td>
</tr>

</table>
<h2>2017</h2>
<table class="table table-hover">

<tr id="tr-amos2017optnet" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
57.
</td>
<td>

<em><a href='http://arxiv.org/abs/1703.00443' target='_blank'>OptNet: Differentiable Optimization as a Layer in Neural Networks</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2017optnet").toggle()'>abs</a>] [<a href='https://github.com/locuslab/optnet' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2017.optnet.pdf' target='_blank'>slides</a>] <br>
<strong>Brandon&nbsp;Amos</strong> and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter</a><br>
ICML 2017  <br>

<div id="abs_amos2017optnet" style="text-align: justify; display: none" markdown="1">
This paper presents OptNet, a network architecture that integrates
optimization problems (here, specifically in the form of quadratic programs)
as individual layers in larger end-to-end trainable deep networks.
These layers encode constraints and complex dependencies
between the hidden states that traditional convolutional and
fully-connected layers often cannot capture.
In this paper, we explore the foundations for such an architecture:
we show how techniques from sensitivity analysis, bilevel
optimization, and implicit differentiation can be used to
exactly differentiate through these layers and with respect
to layer parameters;
we develop a highly efficient solver for these layers that exploits fast
GPU-based batch solves within a primal-dual interior point method, and which
provides backpropagation gradients with virtually no additional cost on top of
the solve;
and we highlight the application of these approaches in several problems.
In one notable example, we show that the method is
capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game;
this highlights the ability of our architecture to learn hard
constraints better than other neural architectures.
</div>

</td>
</tr>


<tr id="tr-amos2017input" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
58.
</td>
<td>

<em><a href='http://arxiv.org/abs/1609.07152' target='_blank'>Input Convex Neural Networks</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2017input").toggle()'>abs</a>] [<a href='https://github.com/locuslab/icnn' target='_blank'>code</a>]  [<a href='http://bamos.github.io/data/slides/2017.icnn.pdf' target='_blank'>slides</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://leixx.io/' target='_blank'>Lei&nbsp;Xu</a>, and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter</a><br>
ICML 2017  <br>

<div id="abs_amos2017input" style="text-align: justify; display: none" markdown="1">
This paper presents the input convex neural network
architecture. These are scalar-valued (potentially deep) neural
networks with constraints on the network parameters such that the
output of the network is a convex function of (some of) the inputs.
The networks allow for efficient inference via optimization over some
inputs to the network given others, and can be applied to settings
including structured prediction, data imputation, reinforcement
learning, and others. In this paper we lay the basic groundwork for
these models, proposing methods for inference, optimization and
learning, and analyze their representational power. We show that many
existing neural network architectures can be made input-convex with
a minor modification, and develop specialized optimization
algorithms tailored to this setting. Finally, we highlight the
performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show
improvement over the existing state of the art in many cases.
</div>

</td>
</tr>


<tr id="tr-donti2017task" >
<td align='right' style='padding-left:0;padding-right:0;'>
59.
</td>
<td>

<em><a href='http://arxiv.org/abs/1703.04529' target='_blank'>Task-based End-to-end Model Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_donti2017task").toggle()'>abs</a>] [<a href='https://github.com/locuslab/e2e-model-learning' target='_blank'>code</a>] <br>
<a href='https://priyadonti.com/' target='_blank'>Priya&nbsp;L.&nbsp;Donti</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://zicokolter.com/' target='_blank'>J.&nbsp;Zico&nbsp;Kolter</a><br>
NeurIPS 2017  <br>

<div id="abs_donti2017task" style="text-align: justify; display: none" markdown="1">
As machine learning techniques have become more ubiquitous, it has
become common to see machine learning prediction algorithms operating
within some larger process. However, the criteria by which we train
machine learning algorithms often differ from the ultimate criteria on
which we evaluate them. This paper proposes an end-to-end approach for
learning probabilistic machine learning models within the context of
stochastic programming, in a manner that directly captures the
ultimate task-based objective for which they will be used. We then
present two experimental evaluations of the proposed approach, one as
applied to a generic inventory stock problem and the second to a
real-world electrical grid scheduling task. In both cases, we show
that the proposed approach can outperform both a traditional modeling
approach and a purely black-box policy optimization approach.
</div>

</td>
</tr>


<tr id="tr-chen2017quasi" >
<td align='right' style='padding-left:0;padding-right:0;'>
60.
</td>
<td>

<em><a href='https://par.nsf.gov/servlets/purl/10111392' target='_blank'>Quasi-Newton Stochastic Optimization Algorithm for Parameter Estimation of a Stochastic Model of the Budding Yeast Cell Cycle</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2017quasi").toggle()'>abs</a>]<br>
<a href='https://chenm.sites.wfu.edu/publications/' target='_blank'>Minghan&nbsp;Chen</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://people.cs.vt.edu/~ltw/shortvita.html' target='_blank'>Layne&nbsp;T.&nbsp;Watson</a>, <a href='https://scholar.google.com/citations?user=syETjMMAAAAJ' target='_blank'>John&nbsp;Tyson</a>, <a href='https://people.cs.vt.edu/~ycao/' target='_blank'>Yang&nbsp;Cao</a>, <a href='https://people.cs.vt.edu/shaffer/' target='_blank'>Cliff&nbsp;Shaffer</a>, <a href='https://mtrosset.pages.iu.edu/' target='_blank'>Michael&nbsp;Trosset</a>, <a href='https://scholar.google.com/citations?user=Z4534DUAAAAJ' target='_blank'>Cihan&nbsp;Oguz</a>, and <a href='https://dblp.org/pid/235/5473.html' target='_blank'>Gisella&nbsp;Kakoti</a><br>
IEEE/ACM TCBB 2017  <br>

<div id="abs_chen2017quasi" style="text-align: justify; display: none" markdown="1">
Parameter estimation in discrete or continuous deterministic cell
cycle models is challenging for several reasons, including the nature of what can be observed, and
the accuracy and quantity of those observations. The
challenge is even greater for stochastic models, where the number of simulations and amount of
empirical data must be even larger to obtain
statistically valid parameter estimates. The two
main contributions of this work are (1) stochastic
model parameter estimation based on directly
matching multivariate probability distributions, and
(2) a new quasi-Newton algorithm class QNSTOP for
stochastic optimization problems. QNSTOP directly
uses the random objective function value samples
rather than creating ensemble statistics. QNSTOP is
used here to directly match empirical and simulated
joint probability distributions rather than matching
summary statistics. Results are given for a current
state-of-the-art stochastic cell cycle model of
budding yeast, whose predictions match well some
summary statistics and one-dimensional distributions
from empirical data, but do not match well the
empirical joint distributions. The nature of the
mismatch provides insight into the weakness in the
stochastic model.
</div>

</td>
</tr>


<tr id="tr-ha2017you" >
<td align='right' style='padding-left:0;padding-right:0;'>
61.
</td>
<td>

<em><a href='https://dl.acm.org/doi/10.1145/3132211.3134453' target='_blank'>You can teach elephants to dance: agile VM handoff for edge computing</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ha2017you").toggle()'>abs</a>]<br>
<a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='https://dblp.org/pid/18/1620.html' target='_blank'>Yoshihisa&nbsp;Abe</a>, <a href='https://dblp.org/pid/207/9122.html' target='_blank'>Thomas&nbsp;Eiszler</a>, <a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://dblp.org/pid/207/9123.html' target='_blank'>Rohit&nbsp;Upadhyaya</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
SEC 2017  <br>

<div id="abs_ha2017you" style="text-align: justify; display: none" markdown="1">
VM handoff enables rapid and transparent placement changes to
executing code in edge computing use cases where the
safety and management attributes of VM encapsulation
are important. This versatile primitive offers the
functionality of classic live migration but is
highly optimized for the edge. Over WAN bandwidths
ranging from 5 to 25 Mbps, VM handoff migrates a
running 8 GB VM in about a minute, with a downtime
of a few tens of seconds. By dynamically adapting to
varying network bandwidth and processing load, VM
handoff is more than an order of magnitude faster
than live migration at those bandwidths.
</div>

</td>
</tr>


<tr id="tr-chen2017empirical" >
<td align='right' style='padding-left:0;padding-right:0;'>
62.
</td>
<td>

<em><a href='https://www.cs.cmu.edu/~zhuoc/papers/latency2017.pdf' target='_blank'>An Empirical Study of Latency in an Emerging Class of Edge Computing Applications for Wearable Cognitive Assistance</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2017empirical").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <a href='https://www.junjuewang.com' target='_blank'>Junjue&nbsp;Wang</a>, <a href='https://scholar.google.com/citations?user=0OpjwCMAAAAJ' target='_blank'>Siyan&nbsp;Zhao</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=0pF6i38AAAAJ' target='_blank'>Guanhang&nbsp;Wu</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='https://scholar.google.com/citations?user=R9r5_GIAAAAJ' target='_blank'>Khalid&nbsp;Elgazzar</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='https://scholar.google.com/citations?user=wUPKh58AAAAJ' target='_blank'>Roberta&nbsp;Klatzky</a>, <a href='http://www.cs.cmu.edu/~dps/' target='_blank'>Daniel&nbsp;Siewiorek</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
SEC 2017  <br>

<div id="abs_chen2017empirical" style="text-align: justify; display: none" markdown="1">
An emerging class of interactive wearable cognitive assistance
applications is poised to become one of the key
demonstrators of edge computing infrastructure. In
this paper, we design seven such applications and
evaluate their performance in terms of latency
across a range of edge computing configurations, mobile hardware, and wireless networks, including 4G
LTE. We also devise a novel multi-algorithm approach
that leverages temporal locality to reduce
end-to-end latency by 60% to 70%, without
sacrificing accuracy. Finally, we derive target
latencies for our applications, and show that edge
computing is crucial to meeting these targets.
</div>

</td>
</tr>


<tr id="tr-wang2017scalable" >
<td align='right' style='padding-left:0;padding-right:0;'>
63.
</td>
<td>

<em><a href='http://elijah.cs.cmu.edu/DOCS/wang-mmsys2017.pdf' target='_blank'>A Scalable and Privacy-Aware IoT Service for Live Video Analytics</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_wang2017scalable").toggle()'>abs</a>] [<a href='http://cmusatyalab.github.io/openface/' target='_blank'>code</a>] <br>
<a href='https://www.junjuewang.com' target='_blank'>Junjue&nbsp;Wang</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://anupamdas.org/' target='_blank'>Anupam&nbsp;Das</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='https://www.normsadeh.org/' target='_blank'>Norman&nbsp;Sadeh</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
ACM MMSys 2017 (Best Paper) <br>

<div id="abs_wang2017scalable" style="text-align: justify; display: none" markdown="1">
We present OpenFace, our new open-source face recognition system
that approaches state-of-the-art accuracy. Integrating OpenFace with
inter-frame tracking, we build RTFace, a mechanism for denaturing video
streams that selectively blurs faces according to specified
policies at full frame rates. This enables privacy management for
live video analytics while providing a secure approach for handling
retrospective policy exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks using RTFace.
</div>

</td>
</tr>

</table>
<h2>2016</h2>
<table class="table table-hover">

<tr id="tr-amos2016openface" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
64.
</td>
<td>

<em><a href='http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf' target='_blank'>OpenFace: A general-purpose face recognition library with mobile applications</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2016openface").toggle()'>abs</a>] [<a href='https://cmusatyalab.github.io/openface' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://www.linkedin.com/in/bartosz-ludwiczuk-a677a760' target='_blank'>Bartosz&nbsp;Ludwiczuk</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
CMU 2016  <br>

<div id="abs_amos2016openface" style="text-align: justify; display: none" markdown="1">
Cameras are becoming ubiquitous in the Internet of Things (IoT) and
can use face recognition technology to improve context. There is a
large accuracy gap between today's publicly available face recognition
systems and the state-of-the-art private face recognition
systems. This paper presents our OpenFace face recognition library
that bridges this accuracy gap. We show that OpenFace provides
near-human accuracy on the LFW benchmark and present a new
classification benchmark for mobile scenarios. This paper is intended
for non-experts interested in using OpenFace and provides a light
introduction to the deep neural network techniques we use.

We released OpenFace in October 2015 as an open source library under
the Apache 2.0 license. It is available at:
<http://cmusatyalab.github.io/openface/>
</div>

</td>
</tr>


<tr id="tr-amos2016deepcompletion" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
65.
</td>
<td>

<em><a href='https://bamos.github.io/2016/08/09/deep-completion/' target='_blank'>Image Completion with Deep Learning in TensorFlow</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2016deepcompletion").toggle()'>abs</a>]<br>
<strong>Brandon&nbsp;Amos</strong><br>
Blog Post 2016  <br>

<div id="abs_amos2016deepcompletion" style="text-align: justify; display: none" markdown="1">
Content-aware fill is a powerful tool designers and photographers use to fill in unwanted or missing parts of images. Image completion and inpainting are closely related technologies used to fill in missing or corrupted parts of images. There are many ways to do content-aware fill, image completion, and inpainting. In this blog post, I present Raymond Yeh and Chen Chen et al.'s paper "Semantic Image Inpainting with Perceptual and Contextual Losses," which was just posted on arXiv on July 26, 2016. This paper shows how to use deep learning for image completion with a DCGAN.
</div>

</td>
</tr>


<tr id="tr-zhao2016collapsed" >
<td align='right' style='padding-left:0;padding-right:0;'>
66.
</td>
<td>

<em><a href='http://proceedings.mlr.press/v48/zhaoa16.html' target='_blank'>Collapsed Variational Inference for Sum-Product Networks</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zhao2016collapsed").toggle()'>abs</a>]<br>
<a href='https://hanzhaoml.github.io/' target='_blank'>Han&nbsp;Zhao</a>, <a href='https://tameemadel.wordpress.com/' target='_blank'>Tameem&nbsp;Adel</a>, <a href='http://www.cs.cmu.edu/~ggordon/' target='_blank'>Geoff&nbsp;Gordon</a>, and <strong>Brandon&nbsp;Amos</strong><br>
ICML 2016  <br>

<div id="abs_zhao2016collapsed" style="text-align: justify; display: none" markdown="1">
Sum-Product Networks (SPNs) are probabilistic inference machines that admit
exact inference in linear time in the size of the network. Existing
parameter learning approaches for SPNs are largely based on the maximum
likelihood principle and hence are subject to overfitting compared to
more Bayesian approaches. Exact Bayesian posterior inference for SPNs is
computationally intractable. Both standard variational inference and
posterior sampling for SPNs are computationally infeasible even for
networks of moderate size due to the large number of local latent
variables per instance. In this work, we propose a novel deterministic
collapsed variational inference algorithm for SPNs that is
computationally efficient, easy to implement and at the same time allows
us to incorporate prior information into the optimization formulation.
Extensive experiments show a significant improvement in accuracy compared
with a maximum likelihood based approach.
</div>

</td>
</tr>


<tr id="tr-hu2016quantifying" >
<td align='right' style='padding-left:0;padding-right:0;'>
67.
</td>
<td>

<em><a href='https://dl.acm.org/doi/10.1145/2967360.2967369' target='_blank'>Quantifying the impact of edge computing on mobile applications</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_hu2016quantifying").toggle()'>abs</a>]<br>
<a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <a href='https://www.linkedin.com/in/joelyinggao/' target='_blank'>Ying&nbsp;Gao</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='https://www.junjuewang.com' target='_blank'>Junjue&nbsp;Wang</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
ACM SIGOPS 2016  <br>

<div id="abs_hu2016quantifying" style="text-align: justify; display: none" markdown="1">
Computational offloading services at the edge of the Internet for
mobile devices are becoming a reality. Using a wide
range of mobile applications, we explore how such
infrastructure improves latency and energy
consumption relative to the cloud. We present
experimental results from WiFi and 4G LTE networks
that confirm substantial wins from edge computing
for highly interactive mobile applications.
</div>

</td>
</tr>


<tr id="tr-davies2016privacy" >
<td align='right' style='padding-left:0;padding-right:0;'>
68.
</td>
<td>

<em><a href='http://eprints.lancs.ac.uk/78255/1/44691.pdf' target='_blank'>Privacy mediators: helping IoT cross the chasm</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_davies2016privacy").toggle()'>abs</a>]<br>
<a href='https://www.lancaster.ac.uk/sci-tech/about-us/people/nigel-davies' target='_blank'>Nigel&nbsp;Davies</a>, <a href='https://scholar.google.com/citations?user=BItCgjYAAAAJ' target='_blank'>Nina&nbsp;Taft</a>, <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a>, <a href='http://www.sclinch.com/' target='_blank'>Sarah&nbsp;Clinch</a>, and <strong>Brandon&nbsp;Amos</strong><br>
HotMobile 2016  <br>

<div id="abs_davies2016privacy" style="text-align: justify; display: none" markdown="1">
Unease over data privacy will retard consumer acceptance of IoT
deployments. The primary source of discomfort is a lack of user
control over raw data that is streamed directly from sensors to the
cloud. This is a direct consequence of the over-centralization of
today’s cloud-based IoT hub designs. We propose a solution that
interposes a locally-controlled software component called a privacy
mediator on every raw sensor stream. Each mediator is in the same
administrative domain as the sensors whose data is being collected, and dynamically enforces the current privacy policies of the owners
of the sensors or mobile users within the domain. This solution necessitates
a logical point of presence for mediators within the administrative
boundaries of each organization. Such points of presence
are provided by cloudlets, which are small locally-administered data
centers at the edge of the Internet that can support code mobility.
The use of cloudlet-based mediators aligns well with natural personal
and organizational boundaries of trust and responsibility.
</div>

</td>
</tr>

</table>
<h2>2015 and earlier</h2>
<table class="table table-hover">

<tr id="tr-satyanarayanan2015edge" >
<td align='right' style='padding-left:0;padding-right:0;'>
69.
</td>
<td>

<em><a href='https://www.cs.cmu.edu/~satya/docdir/satya-edge2015.pdf' target='_blank'>Edge Analytics in the Internet of Things</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_satyanarayanan2015edge").toggle()'>abs</a>]<br>
<a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a>, <a href='https://www.ugent.be/ea/idlab/en/members/pieter-simoens.htm' target='_blank'>Pieter&nbsp;Simoens</a>, <a href='https://scholar.google.com/citations?user=ZeRhyWsAAAAJ' target='_blank'>Yu&nbsp;Xiao</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, and <strong>Brandon&nbsp;Amos</strong><br>
IEEE Pervasive Computing 2015  <br>

<div id="abs_satyanarayanan2015edge" style="text-align: justify; display: none" markdown="1">
High-data-rate sensors, such as video cameras, are becoming ubiquitous in the
Internet of Things. This article describes GigaSight, an Internet-scale
repository of crowd-sourced video content, with strong enforcement of privacy
preferences and access controls. The GigaSight architecture is a federated
system of VM-based cloudlets that perform video analytics at the edge of the
Internet, thus reducing the demand for ingress bandwidth into the cloud.
Denaturing, which is an owner-specific reduction in fidelity of video content
to preserve privacy, is one form of analytics on cloudlets. Content-based
indexing for search is another form of cloudlet-based analytics. This article
is part of a special issue on smart spaces.
</div>

</td>
</tr>


<tr id="tr-turner2015bad" >
<td align='right' style='padding-left:0;padding-right:0;'>
70.
</td>
<td>

<em><a href='http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7118094' target='_blank'>Bad Parts: Are Our Manufacturing Systems at Risk of Silent Cyberattacks?</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_turner2015bad").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=MRKab9cAAAAJ' target='_blank'>Hamilton&nbsp;Turner</a>, <a href='https://scholar.google.com/citations?user=10HSX90AAAAJ' target='_blank'>Jules&nbsp;White</a>, <a href='https://scholar.google.com/citations?user=tWmVBNwAAAAJ' target='_blank'>Jaime&nbsp;A.&nbsp;Camelio</a>, <a href='https://scholar.google.com/citations?user=AW81mosAAAAJ' target='_blank'>Christopher&nbsp;Williams</a>, <strong>Brandon&nbsp;Amos</strong>, and <a href='https://ieeexplore.ieee.org/author/37085729541' target='_blank'>Robert&nbsp;Parker</a><br>
IEEE Security & Privacy 2015  <br>

<div id="abs_turner2015bad" style="text-align: justify; display: none" markdown="1">
Recent cyberattacks have highlighted the risk of physical equipment operating
outside designed tolerances to produce catastrophic failures. A related
threat is cyberattacks that change the design and manufacturing of a
machine's part, such as an automobile brake component, so it no longer
functions properly. These risks stem from the lack of cyber-physical models
to identify ongoing attacks as well as the lack of rigorous application of
known cybersecurity best practices. To protect manufacturing processes in the
future, research will be needed on a number of critical cyber-physical
manufacturing security topics.
</div>

</td>
</tr>


<tr id="tr-chen2015early" >
<td align='right' style='padding-left:0;padding-right:0;'>
71.
</td>
<td>

<em><a href='http://www.cs.cmu.edu/~satya/docdir/chen-wearsys2015.pdf' target='_blank'>Early Implementation Experience with Wearable Cognitive Assistance Applications</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_chen2015early").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://www.lujiang.info/' target='_blank'>Lu&nbsp;Jiang</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='http://www.cs.cmu.edu/~alex/' target='_blank'>Alex&nbsp;Hauptmann</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
WearSys 2015  <br>

<div id="abs_chen2015early" style="text-align: justify; display: none" markdown="1">
A cognitive assistance application combines a wearable device such
as Google Glass with cloudlet processing to provide step-by-step
guidance on a complex task. In this paper, we focus on user assistance
for narrow and well-defined tasks that require specialized
knowledge and/or skills. We describe proof-of-concept implementations
for four different tasks: assembling 2D Lego models, freehand
sketching, playing ping-pong, and recommending context-relevant
YouTube tutorials. We then reflect on the difficulties we faced in
building these applications, and suggest future research that could
simplify the creation of similar applications.
</div>

</td>
</tr>


<tr id="tr-hu2014case" >
<td align='right' style='padding-left:0;padding-right:0;'>
72.
</td>
<td>

<em><a href='http://www.cs.cmu.edu/~satya/docdir/hu-hotmobile2015.pdf' target='_blank'>The Case for Offload Shaping</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_hu2014case").toggle()'>abs</a>]<br>
<a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='https://scholar.google.com/citations?user=vU6bKxEAAAAJ' target='_blank'>Wolfgang&nbsp;Richter</a>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, <a href='https://github.com/bgilbert' target='_blank'>Benjamin&nbsp;Gilbert</a>, <a href='https://scholar.google.com/citations?user=jj5tN8sAAAAJ' target='_blank'>Jan&nbsp;Harkes</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
HotMobile 2015  <br>

<div id="abs_hu2014case" style="text-align: justify; display: none" markdown="1">
When offloading computation from a mobile device, we show
that it can pay to perform additional on-device work in order
to reduce the offloading workload. We call this offload shaping, and demonstrate its application at many different levels
of abstraction using a variety of techniques. We show that
offload shaping can produce significant reduction in resource
demand, with little loss of application-level fidelity
</div>

</td>
</tr>


<tr id="tr-gao2015cloudlets" >
<td align='right' style='padding-left:0;padding-right:0;'>
73.
</td>
<td>

<em><a href='http://reports-archive.adm.cs.cmu.edu/anon/anon/2015/CMU-CS-15-139.pdf' target='_blank'>Are Cloudlets Necessary?</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_gao2015cloudlets").toggle()'>abs</a>]<br>
<a href='https://www.linkedin.com/in/joelyinggao/' target='_blank'>Ying&nbsp;Gao</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
CMU 2015  <br>

<div id="abs_gao2015cloudlets" style="text-align: justify; display: none" markdown="1">
We present experimental results from Wi-Fi and 4G LTE networks to validate the
intuition that low end-to-end latency of cloud services improves application
response time and reduces energy consumption on mobile devices. We focus
specifically on computational offloading as a cloud service. Using a wide
range of applications, and exploring both pre-partitioned and dynamically
partitioned approaches, we demonstrate the importance of low latency for
cloud offload services. We show the best performance is achieved by
offloading to cloudlets, which are small-scale edge-located data centers. Our
results show that cloudlets can improve response times 51% and reduce energy
consumption in a mobile device by up to 42% compared to cloud offload.
</div>

</td>
</tr>


<tr id="tr-ha2015adaptive" >
<td align='right' style='padding-left:0;padding-right:0;'>
74.
</td>
<td>

<em><a href='http://ra.adm.cs.cmu.edu/anon/2015/CMU-CS-15-113.pdf' target='_blank'>Adaptive VM handoff across cloudlets</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ha2015adaptive").toggle()'>abs</a>]<br>
<a href='http://krha.kr/' target='_blank'>Kiryong&nbsp;Ha</a>, <a href='https://dblp.org/pid/18/1620.html' target='_blank'>Yoshihisa&nbsp;Abe</a>, <a href='https://scholar.google.com/citations?user=-wKpBNkAAAAJ' target='_blank'>Zhuo&nbsp;Chen</a>, <a href='http://www.cs.cmu.edu/~wenluh/' target='_blank'>Wenlu&nbsp;Hu</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://www.andrew.cmu.edu/user/pspillai/' target='_blank'>Padmanabhan&nbsp;Pillai</a>, and <a href='https://www.cs.cmu.edu/~satya/' target='_blank'>Mahadev&nbsp;Satyanarayanan</a><br>
CMU 2015  <br>

<div id="abs_ha2015adaptive" style="text-align: justify; display: none" markdown="1">
Cloudlet offload is a valuable technique for ensuring low end-to-end latency of
resource-intensive cloud processing for many emerging mobile applications.
This paper examines the impact of user mobility on cloudlet offload, and
shows that even modest user mobility can result in significant network
degradation. We propose VM handoff as a technique for seamlessly transferring
VM-encapsulated execution to a more optimal offload site as users move. Our
approach can perform handoff in roughly a minute even over limited WANs by
adaptively reducing data transferred. We present experimental results to
validate our implementation and to demonstrate effectiveness of adaptation to
changing network conditions and processing capacity
</div>

</td>
</tr>


<tr id="tr-andrew2014global" >
<td align='right' style='padding-left:0;padding-right:0;'>
75.
</td>
<td>

<em><a href='http://dl.acm.org/citation.cfm?id=2685662' target='_blank'>Global Parameter Estimation for a Eukaryotic Cell Cycle Model in Systems Biology</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_andrew2014global").toggle()'>abs</a>]<br>
<a href='https://scholar.google.com/citations?user=qpRt_KYAAAAJ' target='_blank'>Tricity&nbsp;Andrew</a>, <strong>Brandon&nbsp;Amos</strong>, <a href='https://dblp.org/pid/75/8682.html' target='_blank'>David&nbsp;Easterling</a>, <a href='https://scholar.google.com/citations?user=Z4534DUAAAAJ' target='_blank'>Cihan&nbsp;Oguz</a>, <a href='https://scholar.google.com/citations?user=fAmU38gAAAAJ' target='_blank'>William&nbsp;Baumann</a>, <a href='https://scholar.google.com/citations?user=syETjMMAAAAJ' target='_blank'>John&nbsp;Tyson</a>, and <a href='https://people.cs.vt.edu/~ltw/shortvita.html' target='_blank'>Layne&nbsp;T.&nbsp;Watson</a><br>
SummerSim 2014  <br>

<div id="abs_andrew2014global" style="text-align: justify; display: none" markdown="1">
The complicated process by which a yeast cell divides, known as the cell
cycle, has been modeled by a system of 26 nonlinear ordinary differential
equations (ODEs) with 149 parameters. This model captures the chemical
kinetics of the regulatory networks controlling the cell division process
in budding yeast cells. Empirical data is discrete and matched against
discrete inferences (e.g., whether a particular mutant cell lives or dies)
computed from the ODE solution trajectories. The problem of
estimating the ODE parameters to best fit the model to the data is a
149-dimensional global optimization problem attacked by the deterministic
algorithm VTDIRECT95 and by the nondeterministic algorithms differential
evolution, QNSTOP, and simulated annealing, whose performances are
compared.
</div>

</td>
</tr>


<tr id="tr-amos2013applying" >
<td align='right' style='padding-left:0;padding-right:0;'>
76.
</td>
<td>

<em><a href='http://bamos.github.io/data/papers/amos-iwcmc2013.pdf' target='_blank'>Applying machine learning classifiers to dynamic Android malware detection at scale</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_amos2013applying").toggle()'>abs</a>] [<a href='https://github.com/VT-Magnum-Research/antimalware' target='_blank'>code</a>] <br>
<strong>Brandon&nbsp;Amos</strong>, <a href='https://scholar.google.com/citations?user=MRKab9cAAAAJ' target='_blank'>Hamilton&nbsp;Turner</a>, and <a href='https://scholar.google.com/citations?user=10HSX90AAAAJ' target='_blank'>Jules&nbsp;White</a><br>
IWCMC 2013  <br>

<div id="abs_amos2013applying" style="text-align: justify; display: none" markdown="1">
The widespread adoption and contextually sensitive
nature of smartphone devices has increased concerns over smartphone
malware. Machine learning classifiers are a current method
for detecting malicious applications on smartphone systems. This
paper presents the evaluation of a number of existing classifiers, using a dataset containing thousands of real (i.e. not synthetic)
applications. We also present our STREAM framework, which
was developed to enable rapid large-scale validation of mobile
malware machine learning classifiers.
</div>

</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Open Source Repositories
38.9k+ GitHub stars across all repositories.

<table class="table table-hover">
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>1.</td>
  <td>
    <span class='cvdate'>2025</span>
    <a href="https://github.com/oripress/AlgoTune">oripress/AlgoTune</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;64
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>2.</td>
  <td>
    <span class='cvdate'>2025</span>
    <a href="https://github.com/facebookresearch/adjoint_sampling">facebookresearch/adjoint_sampling</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;122
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>3.</td>
  <td>
    <span class='cvdate'>2025</span>
    <a href="https://github.com/facebookresearch/oni">facebookresearch/oni</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;43
    </span>
 <em style="color: gray;">&mdash; Online LLM intrinsic rewards for NetHack</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>4.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/facebookresearch/advprompter">facebookresearch/advprompter</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;167
    </span>
 <em style="color: gray;">&mdash; Fast Adaptive Adversarial Prompting for LLMs</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>5.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/facebookresearch/lagrangian-ot">facebookresearch/lagrangian-ot</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;59
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>6.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/lazaratan/meta-flow-matching">lazaratan/meta-flow-matching</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;65
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>7.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/facebookresearch/soc-matching">facebookresearch/soc-matching</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;37
    </span>
 <em style="color: gray;">&mdash; Stochastic Optimal Control Matching</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>8.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/kuleshov/cornell-cs5785-2024-applied-ml">kuleshov/cornell-cs5785-2024-applied-ml</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;513
    </span>
 <em style="color: gray;">&mdash; Slides for our applied ML course</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>9.</td>
  <td>
    <span class='cvdate'>2023</span>
    <a href="https://github.com/facebookresearch/amortized-optimization-tutorial">facebookresearch/amortized-optimization-tutorial</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;246
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>10.</td>
  <td>
    <span class='cvdate'>2023</span>
    <a href="https://github.com/facebookresearch/taskmet">facebookresearch/taskmet</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;19
    </span>
 <em style="color: gray;">&mdash; Task-Driven Metric Learning for Model Learning</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>11.</td>
  <td>
    <span class='cvdate'>2023</span>
    <a href="https://github.com/facebookresearch/w2ot">facebookresearch/w2ot</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;47
    </span>
 <em style="color: gray;">&mdash; Wasserstein-2 optimal transport</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>12.</td>
  <td>
    <span class='cvdate'>2023</span>
    <a href="https://github.com/facebookresearch/LANCER">facebookresearch/LANCER</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;36
    </span>
 <em style="color: gray;">&mdash; Landscape Surrogate Learning Decision Losses</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>13.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/facebookresearch/theseus">facebookresearch/theseus</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;2k
    </span>
 <em style="color: gray;">&mdash; Differentiable non-linear optimization library</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>14.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/facebookresearch/meta-ot">facebookresearch/meta-ot</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;104
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>15.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/bamos/presentations">bamos/presentations</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;142
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>16.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/facebookresearch/gwil">facebookresearch/gwil</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;25
    </span>
 <em style="color: gray;">&mdash; Gromov-Wasserstein Cross Domain Imitation Learning</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>17.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/facebookresearch/nocturne">facebookresearch/nocturne</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;287
    </span>
 <em style="color: gray;">&mdash; A partially-observable multi-agent driving simulator</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>18.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/facebookresearch/rcpm">facebookresearch/rcpm</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;67
    </span>
 <em style="color: gray;">&mdash; Riemannian Convex Potential Maps</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>19.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/facebookresearch/svg">facebookresearch/svg</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;57
    </span>
 <em style="color: gray;">&mdash; Model-based stochastic value gradient</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>20.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/facebookresearch/mbrl-lib">facebookresearch/mbrl-lib</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;1k
    </span>
 <em style="color: gray;">&mdash; Model-based reinforcement learning library</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>21.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/martius-lab/CombOptNet">martius-lab/CombOptNet</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;72
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>22.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/samcohen16/Aligning-Time-Series">samcohen16/Aligning-Time-Series</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;51
    </span>
 <em style="color: gray;">&mdash; Aligning time series on incomparable spaces</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>23.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/facebookresearch/neural_stpp">facebookresearch/neural_stpp</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;104
    </span>
 <em style="color: gray;">&mdash; Neural Spatio-Temporal Point Processes</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>24.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/facebookresearch/neural-scs">facebookresearch/neural-scs</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;29
    </span>
 <em style="color: gray;">&mdash; Neural Fixed-Point Acceleration for SCS</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>25.</td>
  <td>
    <span class='cvdate'>2021</span>
    <a href="https://github.com/rtqichen/torchdiffeq">rtqichen/torchdiffeq</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;6.2k
    </span>
 <em style="color: gray;">&mdash; PyTorch Differentiable ODE Solvers (differentiable event handling)</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>26.</td>
  <td>
    <span class='cvdate'>2020</span>
    <a href="https://github.com/facebookresearch/dcem">facebookresearch/dcem</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;124
    </span>
 <em style="color: gray;">&mdash; The Differentiable Cross-Entropy Method</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>27.</td>
  <td>
    <span class='cvdate'>2019</span>
    <a href="https://github.com/facebookresearch/higher">facebookresearch/higher</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;1.6k
    </span>
 <em style="color: gray;">&mdash; PyTorch higher-order gradient and optimization library</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>28.</td>
  <td>
    <span class='cvdate'>2019</span>
    <a href="https://github.com/bamos/thesis">bamos/thesis</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;340
    </span>
 <em style="color: gray;">&mdash; Ph.D. Thesis LaTeX source code</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>29.</td>
  <td>
    <span class='cvdate'>2019</span>
    <a href="https://github.com/cvxgrp/cvxpylayers">cvxgrp/cvxpylayers</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;2k
    </span>
 <em style="color: gray;">&mdash; Differentiable Convex Optimization Layers</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>30.</td>
  <td>
    <span class='cvdate'>2019</span>
    <a href="https://github.com/locuslab/lml">locuslab/lml</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;59
    </span>
 <em style="color: gray;">&mdash; The Limited Multi-Label Projection Layer</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>31.</td>
  <td>
    <span class='cvdate'>2018</span>
    <a href="https://github.com/locuslab/mpc.pytorch">locuslab/mpc.pytorch</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;988
    </span>
 <em style="color: gray;">&mdash; Differentiable PyTorch Model Predictive Control library</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>32.</td>
  <td>
    <span class='cvdate'>2018</span>
    <a href="https://github.com/locuslab/differentiable-mpc">locuslab/differentiable-mpc</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;298
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>33.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/locuslab/icnn">locuslab/icnn</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;300
    </span>
 <em style="color: gray;">&mdash; Input Convex Neural Network experiments</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>34.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/locuslab/optnet">locuslab/optnet</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;559
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>35.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/locuslab/qpth">locuslab/qpth</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;761
    </span>
 <em style="color: gray;">&mdash; Differentiable PyTorch QP solver</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>36.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/bamos/densenet.pytorch">bamos/densenet.pytorch</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;840
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>37.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/bamos/block">bamos/block</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;310
    </span>
 <em style="color: gray;">&mdash; Intelligent block matrix constructions</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>38.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/bamos/setGPU">bamos/setGPU</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;111
    </span>
 <em style="color: gray;">&mdash; Automatically use the least-loaded GPU</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>39.</td>
  <td>
    <span class='cvdate'>2016</span>
    <a href="https://github.com/bamos/dcgan-completion.tensorflow">bamos/dcgan-completion.tensorflow</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;1.3k
    </span>
 <em style="color: gray;">&mdash; Image completion with GANs</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>40.</td>
  <td>
    <span class='cvdate'>2015</span>
    <a href="https://github.com/cmusatyalab/openface">cmusatyalab/openface</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;15.4k
    </span>
 <em style="color: gray;">&mdash; Face recognition with deep neural networks</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>41.</td>
  <td>
    <span class='cvdate'>2015</span>
    <a href="https://github.com/bamos/girl">bamos/girl</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;70
    </span>
 <em style="color: gray;">&mdash; GitHub README link checker</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>42.</td>
  <td>
    <span class='cvdate'>2015</span>
    <a href="https://github.com/bamos/conference-tracker">bamos/conference-tracker</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;71
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>43.</td>
  <td>
    <span class='cvdate'>2014</span>
    <a href="https://github.com/vtopt/qnstop">vtopt/qnstop</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;10
    </span>
 <em style="color: gray;">&mdash; Fortran quasi-Newton stochastic optimization library</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>44.</td>
  <td>
    <span class='cvdate'>2014</span>
    <a href="https://github.com/bamos/snowglobe">bamos/snowglobe</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;28
    </span>
 <em style="color: gray;">&mdash; Haskell-driven, self-hosted web analytics</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>45.</td>
  <td>
    <span class='cvdate'>2014</span>
    <a href="https://github.com/bamos/zsh-history-analysis">bamos/zsh-history-analysis</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;242
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>46.</td>
  <td>
    <span class='cvdate'>2014</span>
    <a href="https://github.com/bamos/beamer-snippets">bamos/beamer-snippets</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;110
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>47.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/bamos/latex-templates">bamos/latex-templates</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;366
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>48.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/cparse/cparse">cparse/cparse</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;360
    </span>
 <em style="color: gray;">&mdash; C++ expression parser using Dijkstra's shunting-yard algorithm</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>49.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/bamos/cv">bamos/cv</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;412
    </span>
 <em style="color: gray;">&mdash; Source for this CV: creates LaTeX/Markdown from YAML/BibTeX</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>50.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/bamos/parsec-benchmark">bamos/parsec-benchmark</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;121
    </span>
 <em style="color: gray;">&mdash; PARSEC benchmark support for Arch Linux</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>51.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/bamos/python-scripts">bamos/python-scripts</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;196
    </span>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>52.</td>
  <td>
    <span class='cvdate'>2013</span>
    <a href="https://github.com/bamos/reading-list">bamos/reading-list</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;186
    </span>
 <em style="color: gray;">&mdash; YAML reading list and notes system</em>  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>53.</td>
  <td>
    <span class='cvdate'>2012</span>
    <a href="https://github.com/bamos/dotfiles">bamos/dotfiles</a>
    <span style="white-space: nowrap">
    | <i class="fa fas fa-star"></i>&nbsp;237
    </span>
 <em style="color: gray;">&mdash; <i class="fa fas fa-heart"></i> Linux, xmonad, emacs, vim, zsh, tmux</em>  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Invited Talks
Slides for my major presentations are available
[here](https://bamos.github.io/presentations/)
under a CC-BY license.

<table class="table table-hover">
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>1.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
     <a href="https://bamos.github.io/presentations/2025/09/30/advprompter-algotune.html"><em>On meta prompt optimization and coding agents</em></a> &mdash;
        <a href="https://www.simonsfoundation.org/event/machine-learning-at-the-flatiron-institute-brandon-amos/">Flatiron ML Seminar</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>2.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
     <a href="https://bamos.github.io/presentations/2025/05/09/advprompter.html"><em>AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs</em></a> &mdash;
        USC
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>3.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
     <a href="https://bamos.github.io/presentations/2025/05/01/amortization-rl-bio.html"><em>On amortized optimization for RL, Bayesian optimization, and biology</em></a> &mdash;
        <a href="https://ai4b.io">ai4b.io</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>4.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
     <a href="https://bamos.github.io/presentations/2024/11/05/transport-between-distributions-over-distributions.html"><em>Transport and flows between distributions over distributions</em></a> &mdash;
        Columbia University
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>5.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/11/05/transport-between-distributions-over-distributions.html"><em>Transport and flows between distributions over distributions</em></a> &mdash;
        Genesis Therapeutics
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>6.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/11/05/transport-between-distributions-over-distributions.html"><em>Transport and flows between distributions over distributions</em></a> &mdash;
        UT Austin
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>7.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/10/27/prompt-optimization-and-amortization.html"><em>On LLM prompt optimization and amortization</em></a> &mdash;
        <a href="https://www.dagstuhl.de/24441">Dagstuhl Seminar on ML for CO</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>8.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/07/01/amortized-optimization-for-OT-and-LLMs.html"><em>Amortized optimization for optimal transport and LLM attacks</em></a> &mdash;
        ISMP
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>9.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/07/01/differentiable-optimization-and-robotics.html"><em>Differentiable optimization for robotics</em></a> &mdash;
        <a href="https://sites.google.com/robotics.utias.utoronto.ca/frontiers-optimization-rss24/schedule">RSS Optimization for Robotics Workshop</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>10.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/07/01/amortized-optimization-and-AI.html"><em>Amortized optimization-based reasoning for AI</em></a> &mdash;
        University of Amsterdam
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>11.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024</span>
     <a href="https://bamos.github.io/presentations/2024/02/01/e2e-geometries.html"><em>End-to-end learning geometries for graphs, dynamical systems, and regression</em></a> &mdash;
        <a href="https://logmeetupnyc.github.io/">LoG New York</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>12.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/amortized-optimal-transport.html"><em>Amortized optimization for optimal transport</em></a> &mdash;
        <a href="https://otmlworkshop.github.io/schedule/">NeurIPS Optimal Transport and ML Workshop</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>13.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/control-learning.html"><em>On optimal control and machine learning</em></a> &mdash;
        <a href="https://frontiers4lcd.github.io/">ICML Control and Dynamical Systems Workshop</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>14.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/amortized-optimization.html"><em>Tutorial on amortized optimization</em></a> &mdash;
        Brown University
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>15.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        NYU AI Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>16.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        Vanderbilt ML Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>17.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        Microsoft Research
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>18.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/amortized-optimal-transport.html"><em>Amortized optimization for computing optimal transport maps</em></a> &mdash;
        <a href="https://sites.google.com/view/sampling-transport-diffusions/home">Flatiron Workshop</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>19.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        Cornell AI Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>20.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        Cornell Tech Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>21.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2023/01/01/differentiable-amortized-optimization.html"><em>Learning with differentiable and amortized optimization</em></a> &mdash;
        Argonne National Laboratory
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>22.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://slideslive.com/38992360/theseus-a-library-for-differentiable-nonlinear-optimization"><em>Theseus: A library for differentiable nonlinear optimization</em></a> &mdash;
        NYU
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>23.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://slideslive.com/38992360/theseus-a-library-for-differentiable-nonlinear-optimization"><em>Theseus: A library for differentiable nonlinear optimization</em></a> &mdash;
        University of Zurich
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>24.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Colorado Mines AMS Colloquium
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>25.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization</em></a> &mdash;
        <a href="https://guaguakai.github.io/IJCAI22-differentiable-optimization/">IJCAI Tutorial</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>26.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-control.html"><em>Differentiable optimization for control and RL</em></a> &mdash;
        <a href="https://darl-workshop.github.io/">ICML Workshop on Decision Awareness in RL</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>27.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        <a href="https://sites.google.com/usc.edu/cpaior-2022/master_class">CPAIOR Master Class</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>28.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/amortized-optimization.html"><em>Tutorial on amortized optimization</em></a> &mdash;
        ICCOPT
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>29.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-control.html"><em>Differentiable optimization for control and RL</em></a> &mdash;
        Gridmatic
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>30.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2021</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-control.html"><em>Learning for control with differentiable optimization and ODEs</em></a> &mdash;
        Columbia University
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>31.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2021</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        IBM Research
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>32.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-control.html"><em>Differentiable optimization for control</em></a> &mdash;
        Max Planck Institute (Tübingen)
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>33.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Mila Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>34.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Deep Declarative Networks</em></a> &mdash;
        <a href="https://anucvml.github.io/ddn-eccvt2020/">ECCV Tutorial</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>35.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>On differentiable optimization for control and vision</em></a> &mdash;
        <a href="https://anucvml.github.io/ddn-cvprw2020/">CVPR Deep Declarative Networks Workshop</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>36.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        <a href="https://sites.google.com/view/cs-159-spring-2020/lectures">Caltech CS 159 (Guest Lecture)</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>37.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
     <a href="https://bamos.github.io/presentations/2020/01/01/dcem.html"><em>Unrolled optimization for learning deep energy models</em></a> &mdash;
        <a href="https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=67922">SIAM MDS Minisymposium</a>
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>38.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2019</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        NYU CILVR Seminar
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>39.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2019</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        INFORMS
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>40.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2019</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Facebook AI Research
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>41.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        ISMP
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>42.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Google Brain
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>43.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Bosch Center for AI
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>44.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Waymo Research
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>45.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Tesla AI
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>46.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        NVIDIA Robotics
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>47.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        Salesforce Research
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>48.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        OpenAI
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>49.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-optimization.html"><em>Differentiable optimization-based modeling for machine learning</em></a> &mdash;
        NNAISENSE
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>50.</td>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
     <a href="https://bamos.github.io/presentations/2022/01/01/differentiable-control.html"><em>Differentiable optimization and control</em></a> &mdash;
        UC Berkeley
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Interns and Students
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
        <a href="https://gaoyuezhou.github.io/">Kathy Zhou</a> (visiting FAIR from NYU)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
        <a href="https://upiterbarg.github.io/">Ulyana Piterbarg</a> (visiting FAIR from NYU)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
        <a href="https://ollieliu.com/">Ollie Liu</a> (visiting FAIR from USC)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2025</span>
        <a href="https://doronhav.github.io/">Doron Haviv</a> (MSKCC PhD committee, now at Genentech)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2024&nbsp;-&nbsp;2025</span>
        <a href="https://scholar.google.com/citations?user=HBAXF6YAAAAJ">Aaron Havens</a> (visiting FAIR from UIUC, now postdoc at FAIR)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022&nbsp;-&nbsp;2024</span>
        <a href="https://arampooladian.com/">Aram-Alexandre Pooladian</a> (visiting FAIR from NYU, now at Yale)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022&nbsp;-&nbsp;2024</span>
        <a href="https://cdenrich.github.io/">Carles Domingo-Enrich</a> (visiting FAIR from NYU, now at MSR)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023&nbsp;-&nbsp;2024</span>
        <a href="https://scholar.google.com/citations?user=njZL5CQAAAAJ">Anselm Paulus</a> (visiting FAIR from Max Planck Institute, Tübingen)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2023</span>
        <a href="https://www.mhr.ai">Matthew Retchin</a> (Columbia MS thesis committee, now at Harvard)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022&nbsp;-&nbsp;2023</span>
        <a href="https://sanaelotfi.github.io/">Sanae Lotfi</a> (visiting FAIR from NYU, now scientist at FAIR)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2022&nbsp;-&nbsp;2023</span>
        <a href="https://dishank-b.github.io">Dishank Bansal</a> (AI resident at FAIR, now at the UK AI Safety Institute)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2021&nbsp;-&nbsp;2022</span>
        <a href="https://www.linkedin.com/in/arnaudfickinger/">Arnaud Fickinger</a> (visiting FAIR from Berkeley)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020&nbsp;-&nbsp;2022</span>
        <a href="https://aaronlou.com/">Aaron Lou</a> (visiting FAIR from Cornell and Stanford, now scientist at OpenAI)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2021&nbsp;-&nbsp;2022</span>
        <a href="https://www.eugenevinitsky.com">Eugene Vinitsky</a> (visiting FAIR from Berkeley, now professor at NYU)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2021&nbsp;-&nbsp;2022</span>
        <a href="https://scholar.google.com/citations?user=CmdjfTsAAAAJ">Samuel Cohen</a> (visiting FAIR from UCL, now CEO at FairGen)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
        <a href="http://www.cs.toronto.edu/~rtqichen/">Ricky Chen</a> (visiting FAIR from Toronto, now scientist at FAIR)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2020</span>
        <a href="http://www.cs.cmu.edu/~pliang/">Paul Liang</a> (visiting FAIR from CMU, now professor at MIT)
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
    <span class='cvdate'>2018</span>
        <a href="https://phillipkwang.com/">Phillip Wang</a> (at CMU, now CEO at <a href="https://gather.town/" target="_blank">Gather</a>)
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Professional Activities
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2026</span>
      AAAI Senior Program Committee
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2025</span>
      NeurIPS Area Chair
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2025</span>
      AAAI Senior Program Committee
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2024</span>
      NeurIPS Area Chair
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2024</span>
      NeurIPS Datasets and Benchmarks Area Chair
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2024</span>
      AAAI Senior Program Committee
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2023</span>
      NeurIPS Area Chair
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2023</span>
      NeurIPS Datasets and Benchmarks Area Chair
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2023</span>
      AAAI Senior Program Committee
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2020</span>
     <a href="https://sites.google.com/view/lmca2020/home">NeurIPS Learning Meets Combinatorial Optimization Workshop Organizer</a>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2020</span>
     <a href="https://anucvml.github.io/ddn-cvprw2020/">CVPR Deep Declarative Networks Workshop Organizer</a>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2020</span>
     <a href="https://anucvml.github.io/ddn-eccvt2020/">ECCV Deep Declarative Networks Tutorial Organizer</a>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
  <span class='cvdate'>2014&nbsp;-&nbsp;2015</span>
      CMU CSD MS Admissions
  </td>
</tr>
</table>

### Reviewing
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>AAAI Conference on Artificial Intelligence</td>
</tr>
<tr>
  <td style='padding-right:0;'>American Controls Conference (ACC)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Artificial Intelligence and Statistics (AISTATS)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE Conference on Decision and Control (CDC)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE Control Systems Letters (L-CSS)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE International Conference on Computer Vision (ICCV)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE International Conference on Intelligent Robots and Systems (IROS)</td>
</tr>
<tr>
  <td style='padding-right:0;'>IEEE International Conference on Robotics and Automation (ICRA)</td>
</tr>
<tr>
  <td style='padding-right:0;'>International Conference on Learning Representations (ICLR)</td>
</tr>
<tr>
  <td style='padding-right:0;'>International Conference on Learning Representations (ICLR) Blog Posts</td>
</tr>
<tr>
  <td style='padding-right:0;'>International Conference on Machine Learning (ICML)</td>
</tr>
<tr>
  <td style='padding-right:0;'>International Conference on Machine Learning (ICML) SODS Workshop</td>
</tr>
<tr>
  <td style='padding-right:0;'>International Conference on the Constraint Programming, AI, and Operations Research (CPAIOR)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Journal of Machine Learning Research (JMLR)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Learning for Dynamics and Control (L4DC)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Mathematical Programming Computation (MPC)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Neural Information Processing Systems (NeurIPS)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track</td>
</tr>
<tr>
  <td style='padding-right:0;'>Neural Information Processing Systems (NeurIPS) Deep RL Workshop</td>
</tr>
<tr>
  <td style='padding-right:0;'>Neural Information Processing Systems (NeurIPS) DiffCVGP Workshop</td>
</tr>
<tr>
  <td style='padding-right:0;'>Neural Information Processing Systems (NeurIPS) OPT Workshop</td>
</tr>
<tr>
  <td style='padding-right:0;'>Optimization Letters</td>
</tr>
<tr>
  <td style='padding-right:0;'>Transactions on Machine Learning Research (TMLR)</td>
</tr>
<tr>
  <td style='padding-right:0;'>Uncertainty in Artificial Intelligence (UAI)</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Teaching
<table class="table table-hover">
<tr>
  <td style='padding-right:0'><strong>Applied Machine Learning</strong>, <em>Cornell Tech CS5785</em>, Co-instructor</td>
  <td class='col-md-2' style='text-align:right; padding-left:0;'>F2024</td>
</tr>
<tr>
  <td style='padding-right:0'><strong>Graduate AI</strong>, <em>CMU 15-780</em>, TA</td>
  <td class='col-md-2' style='text-align:right; padding-left:0;'>S2017</td>
</tr>
<tr>
  <td style='padding-right:0'><strong>Distributed Systems</strong>, <em>CMU 15-640</em>, TA</td>
  <td class='col-md-2' style='text-align:right; padding-left:0;'>S2016</td>
</tr>
<tr>
  <td style='padding-right:0'><strong>Software Design and Data Structures</strong>, <em>VT CS2114</em>, TA</td>
  <td class='col-md-2' style='text-align:right; padding-left:0;'>S2013</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Skills
<table class="table table-hover">
<tr>
  <td class='col-md-2'><strong>Programming</strong></td>
  <td>
C, C++, Fortran, Haskell, Java, Lua, Make, Mathematica, Python, R, Scala
  </td>
</tr>
<tr>
  <td class='col-md-2'><strong>Frameworks</strong></td>
  <td>
JAX, NumPy, Pandas, PyTorch, SciPy, TensorFlow, Torch7
  </td>
</tr>
<tr>
  <td class='col-md-2'><strong>Toolbox</strong></td>
  <td>
Linux, emacs, vim, evil, org, mu4e, xmonad, git, tmux, zsh
  </td>
</tr>
</table>




<p style="font-size: 0.85em; color: #999;">Last updated on October 19, 2025.</p>
